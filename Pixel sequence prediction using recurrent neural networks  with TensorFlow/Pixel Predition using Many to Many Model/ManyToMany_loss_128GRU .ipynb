{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Many to Many Models:128 unit\n",
    "Here we predict the next pixel for each pixel in the images and calculate the cross entropy of each singular pixel against what the ground truth. Then the loss is summed up for every pixel. Note that there is only ground truth matches for the first 783 pixel predictions since the last output will be out of the input comparison range.\n",
    "\n",
    "The GRU cell is used as from task one it appeared to be more stable and give more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Need to load the MNist data to work with\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)\n",
    "# one hot true gives the y labels as vectors with 1's which correspond to the number it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 10 # number of digits\n",
    "batch_size = 100\n",
    "chunk_size = 1 # feeding in pixel by pixel\n",
    "n_chunks = 784 # number of pixels\n",
    "rnn_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Placeholders to store the data. This shape is used to match the tf rnn cell input\n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size],name='InputData')\n",
    "y = tf.placeholder('float',name='LabelData')\n",
    "\n",
    "# The ground truth of the pixels 2 to end to compare against predicted\n",
    "true_pixels = tf.reshape(x, [-1,n_chunks])\n",
    "true_pixels = true_pixels[:,1:]\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the varibles that will be used in to transform the 32d layer to 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([rnn_size,1]))\n",
    "biases=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784, 128)\n",
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "# Here the gru cell is defined of specified size\n",
    "gru_cell = tf.nn.rnn_cell.GRUCell(rnn_size)\n",
    "\n",
    "# The ouputs are a tensor of all the ouput states of the pixels\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = gru_cell, inputs = x,dtype=tf.float32)\n",
    "\n",
    "# Checking to make sure of the correct shape\n",
    "print(outputs.get_shape())\n",
    "print(states.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outputs are reshaped in order to have in (batch*784,RNN_size) for easy matrix multiplication.\n",
    "outputs = tf.reshape(outputs, [-1,rnn_size])\n",
    "\n",
    "# linear transformation\n",
    "linear = tf.matmul(outputs,weights) + biases\n",
    "\n",
    "# Need to reshape so that we have (batch_size, num_pixels)\n",
    "pixel_pred = tf.reshape(linear,[-1,n_chunks])\n",
    "\n",
    "# Since the output is a prediction for the next state, the first 783 is taken since that is the number of GT pixels \n",
    "# available for comparisson\n",
    "pixel_pred = pixel_pred[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the tf function to calculate the Xent to avoid numerical underflow, it applies a sigmoid layer before \n",
    "# calculating the loss.\n",
    "loss =  tf.nn.sigmoid_cross_entropy_with_logits(pixel_pred,true_pixels)\n",
    "\n",
    "# Sum up the loss from each pixel to get the loss of an image\n",
    "loss = tf.reduce_sum(loss,1)\n",
    "\n",
    "# See the sum of the loss rather than the mean\n",
    "loss_test = tf.reduce_sum(loss)\n",
    "\n",
    "# Get the mean image cost and optimize over that.\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/Task2/gru128'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir):\n",
    "    os.makedirs(save_MDir)\n",
    "\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(images, threshold=0.1):\n",
    "    return (threshold < images).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver2 = tf.train.Saver()\n",
    "\n",
    "# Suggested Directory to use\n",
    "save2_MDir = 'models/Task2/gru128/best'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save2_MDir):\n",
    "    os.makedirs(save2_MDir)\n",
    "\n",
    "save_model2 = os.path.join(save2_MDir,'best_accuracy_3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer function\n",
    "Here the main work is done. Each batch is passed through and outputs the cost at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(hm_epochs, start_epoch):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        count = 0\n",
    "        cost_list=[]\n",
    "        start_epoch_t = time.time()\n",
    "        freq_epoch = hm_epochs/hm_epochs\n",
    "        # For each epoch loop over all batches and optimize the cost and produce the test cost\n",
    "        for epoch in range(hm_epochs):\n",
    "            print(\"-------Running Epoch:{}-------\".format(epoch+1+start_epoch))\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            start = time.time()\n",
    "            n_batches = int(mnist.train.num_examples/batch_size)\n",
    "            freq = int(n_batches/5)\n",
    "            # print batch test and train costs.\n",
    "            for i in range(n_batches):\n",
    "                # Get the batches ready and into the correct form and shape\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                epoch_x = binarize(epoch_x)\n",
    "                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})    \n",
    "                epoch_loss += c\n",
    "                if i% freq ==0 or i == (n_batches):\n",
    "                    \n",
    "                    print(\"Trained {} batches with current epoch cost: {}\".format(i+1,epoch_loss))\n",
    "                    print(\"Last batch average cost: \", c)\n",
    "                    print(\"Current run time is: {} \\n\".format(time.time()-start_epoch_t))\n",
    "            # save after every epoch        \n",
    "            saver.save(sess= sess, save_path = save_model)\n",
    "        \n",
    "                        \n",
    "        # calculate the test accuracy and cost\n",
    "        Final_cost_test = sess.run(cost,feed_dict = {x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})\n",
    "    \n",
    "        \n",
    "        saver.save(sess= sess, save_path = save_model)\n",
    "        \n",
    "        print(\"At final epoch: {}, the is cost {}\".format(start_epoch+epoch+1, Final_cost_test))\n",
    "    print(\"Total time taken for run : {:f}\".format(time.time()-start_epoch_t))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Running Epoch:1-------\n",
      "Trained 1 batches with current epoch cost: 679.6242065429688\n",
      "Last batch average cost:  679.624\n",
      "Current run time is: 2.0145812034606934 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 22703.116523742676\n",
      "Last batch average cost:  127.03\n",
      "Current run time is: 213.5842583179474 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 36420.225440979004\n",
      "Last batch average cost:  119.035\n",
      "Current run time is: 449.71931171417236 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 48502.578125\n",
      "Last batch average cost:  97.3191\n",
      "Current run time is: 722.307254076004 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 59000.291343688965\n",
      "Last batch average cost:  89.5\n",
      "Current run time is: 950.8234283924103 \n",
      "\n",
      "-------Running Epoch:2-------\n",
      "Trained 1 batches with current epoch cost: 90.66071319580078\n",
      "Last batch average cost:  90.6607\n",
      "Current run time is: 1181.7898008823395 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 9566.454467773438\n",
      "Last batch average cost:  88.0757\n",
      "Current run time is: 1405.654441356659 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 18771.976928710938\n",
      "Last batch average cost:  82.1158\n",
      "Current run time is: 1660.6639697551727 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 27801.423385620117\n",
      "Last batch average cost:  78.9882\n",
      "Current run time is: 2030.336349248886 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 36664.86404418945\n",
      "Last batch average cost:  80.5578\n",
      "Current run time is: 2262.1341269016266 \n",
      "\n",
      "-------Running Epoch:3-------\n",
      "Trained 1 batches with current epoch cost: 77.18731689453125\n",
      "Last batch average cost:  77.1873\n",
      "Current run time is: 2547.2425396442413 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8623.495750427246\n",
      "Last batch average cost:  73.4464\n",
      "Current run time is: 2882.2221562862396 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17016.503051757812\n",
      "Last batch average cost:  77.1046\n",
      "Current run time is: 3158.262217283249 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25313.598709106445\n",
      "Last batch average cost:  73.7157\n",
      "Current run time is: 3434.8323361873627 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33557.8759765625\n",
      "Last batch average cost:  79.1694\n",
      "Current run time is: 3844.481855392456 \n",
      "\n",
      "-------Running Epoch:4-------\n",
      "Trained 1 batches with current epoch cost: 72.65702819824219\n",
      "Last batch average cost:  72.657\n",
      "Current run time is: 4106.191385746002 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8163.990058898926\n",
      "Last batch average cost:  69.1324\n",
      "Current run time is: 4462.766005277634 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16234.964767456055\n",
      "Last batch average cost:  70.2618\n",
      "Current run time is: 4694.724272012711 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24208.795387268066\n",
      "Last batch average cost:  74.9528\n",
      "Current run time is: 4926.463025808334 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32191.07261657715\n",
      "Last batch average cost:  74.8114\n",
      "Current run time is: 5161.616745233536 \n",
      "\n",
      "-------Running Epoch:5-------\n",
      "Trained 1 batches with current epoch cost: 70.88298034667969\n",
      "Last batch average cost:  70.883\n",
      "Current run time is: 5390.07981801033 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7948.992027282715\n",
      "Last batch average cost:  69.8088\n",
      "Current run time is: 5616.863373756409 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 15781.639686584473\n",
      "Last batch average cost:  72.3827\n",
      "Current run time is: 5845.0897006988525 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 23584.85427093506\n",
      "Last batch average cost:  71.1721\n",
      "Current run time is: 6079.348606348038 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 31379.918258666992\n",
      "Last batch average cost:  72.0494\n",
      "Current run time is: 6319.116682767868 \n",
      "\n",
      "-------Running Epoch:6-------\n",
      "Trained 1 batches with current epoch cost: 68.6283950805664\n",
      "Last batch average cost:  68.6284\n",
      "Current run time is: 6553.4082589149475 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7781.998878479004\n",
      "Last batch average cost:  69.2348\n",
      "Current run time is: 6791.087993621826 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 15474.913963317871\n",
      "Last batch average cost:  64.5889\n",
      "Current run time is: 7071.98740696907 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 23168.370109558105\n",
      "Last batch average cost:  71.5008\n",
      "Current run time is: 7290.9201583862305 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 30787.03140258789\n",
      "Last batch average cost:  70.7927\n",
      "Current run time is: 7508.521897315979 \n",
      "\n",
      "-------Running Epoch:7-------\n",
      "Trained 1 batches with current epoch cost: 68.08024597167969\n",
      "Last batch average cost:  68.0802\n",
      "Current run time is: 7739.52348446846 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7698.978134155273\n",
      "Last batch average cost:  67.9957\n",
      "Current run time is: 7966.74946141243 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 15281.695091247559\n",
      "Last batch average cost:  68.927\n",
      "Current run time is: 8196.380777359009 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 22794.97719192505\n",
      "Last batch average cost:  68.43\n",
      "Current run time is: 8424.716711759567 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 30300.996841430664\n",
      "Last batch average cost:  66.1377\n",
      "Current run time is: 8649.80748128891 \n",
      "\n",
      "-------Running Epoch:8-------\n",
      "Trained 1 batches with current epoch cost: 67.57110595703125\n",
      "Last batch average cost:  67.5711\n",
      "Current run time is: 8877.534400939941 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7561.604549407959\n",
      "Last batch average cost:  67.7567\n",
      "Current run time is: 9168.07668185234 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 15021.118396759033\n",
      "Last batch average cost:  64.8788\n",
      "Current run time is: 9465.316717147827 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 22446.800048828125\n",
      "Last batch average cost:  67.5166\n",
      "Current run time is: 9770.974400520325 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 29843.415477752686\n",
      "Last batch average cost:  65.8637\n",
      "Current run time is: 10001.657413721085 \n",
      "\n",
      "-------Running Epoch:9-------\n",
      "Trained 1 batches with current epoch cost: 68.16641235351562\n",
      "Last batch average cost:  68.1664\n",
      "Current run time is: 10225.702097654343 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7420.255035400391\n",
      "Last batch average cost:  66.0136\n",
      "Current run time is: 10454.367074012756 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14770.129734039307\n",
      "Last batch average cost:  70.1814\n",
      "Current run time is: 10681.333791255951 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 22089.71865081787\n",
      "Last batch average cost:  66.207\n",
      "Current run time is: 11321.440784692764 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 29380.190643310547\n",
      "Last batch average cost:  64.2298\n",
      "Current run time is: 11546.06377029419 \n",
      "\n",
      "-------Running Epoch:10-------\n",
      "Trained 1 batches with current epoch cost: 65.63006591796875\n",
      "Last batch average cost:  65.6301\n",
      "Current run time is: 12538.975354909897 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7314.264320373535\n",
      "Last batch average cost:  64.5155\n",
      "Current run time is: 12777.06710600853 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14527.35424041748\n",
      "Last batch average cost:  62.592\n",
      "Current run time is: 13000.681143760681 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 21781.95273208618\n",
      "Last batch average cost:  67.9291\n",
      "Current run time is: 13226.601384878159 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 29028.691944122314\n",
      "Last batch average cost:  65.0843\n",
      "Current run time is: 13492.560025215149 \n",
      "\n",
      "-------Running Epoch:11-------\n",
      "Trained 1 batches with current epoch cost: 62.6324577331543\n",
      "Last batch average cost:  62.6325\n",
      "Current run time is: 13786.87379360199 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7188.250072479248\n",
      "Last batch average cost:  66.5509\n",
      "Current run time is: 14017.546003341675 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14422.751399993896\n",
      "Last batch average cost:  68.3411\n",
      "Current run time is: 14248.167321443558 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 21609.632472991943\n",
      "Last batch average cost:  63.6867\n",
      "Current run time is: 14480.377620458603 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 28757.755123138428\n",
      "Last batch average cost:  65.812\n",
      "Current run time is: 14714.82035279274 \n",
      "\n",
      "-------Running Epoch:12-------\n",
      "Trained 1 batches with current epoch cost: 64.97624206542969\n",
      "Last batch average cost:  64.9762\n",
      "Current run time is: 14938.361062288284 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7197.795581817627\n",
      "Last batch average cost:  69.4465\n",
      "Current run time is: 15156.647258520126 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14301.045055389404\n",
      "Last batch average cost:  64.6332\n",
      "Current run time is: 15374.239917039871 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 21408.30863571167\n",
      "Last batch average cost:  63.6868\n",
      "Current run time is: 15613.173389911652 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 28516.595867156982\n",
      "Last batch average cost:  64.2714\n",
      "Current run time is: 15842.596086263657 \n",
      "\n",
      "-------Running Epoch:13-------\n",
      "Trained 1 batches with current epoch cost: 63.839759826660156\n",
      "Last batch average cost:  63.8398\n",
      "Current run time is: 16059.472228050232 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7171.794532775879\n",
      "Last batch average cost:  62.7651\n",
      "Current run time is: 16275.757136583328 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14233.787273406982\n",
      "Last batch average cost:  64.929\n",
      "Current run time is: 16491.00306248665 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 21299.52244949341\n",
      "Last batch average cost:  64.5684\n",
      "Current run time is: 16705.95066189766 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 28307.965366363525\n",
      "Last batch average cost:  65.4499\n",
      "Current run time is: 16921.30211853981 \n",
      "\n",
      "-------Running Epoch:14-------\n",
      "Trained 1 batches with current epoch cost: 66.49283599853516\n",
      "Last batch average cost:  66.4928\n",
      "Current run time is: 17135.554907798767 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7082.59094619751\n",
      "Last batch average cost:  61.1388\n",
      "Current run time is: 17352.470953464508 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14090.287784576416\n",
      "Last batch average cost:  66.5332\n",
      "Current run time is: 17568.72354745865 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 21109.328819274902\n",
      "Last batch average cost:  64.6749\n",
      "Current run time is: 17784.90171098709 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 28129.22634124756\n",
      "Last batch average cost:  64.1517\n",
      "Current run time is: 18000.998918771744 \n",
      "\n",
      "-------Running Epoch:15-------\n",
      "Trained 1 batches with current epoch cost: 66.32250213623047\n",
      "Last batch average cost:  66.3225\n",
      "Current run time is: 18221.489906311035 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7084.549331665039\n",
      "Last batch average cost:  63.8814\n",
      "Current run time is: 18438.018087863922 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 14049.55585861206\n",
      "Last batch average cost:  65.0974\n",
      "Current run time is: 18655.72478413582 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 21005.05726623535\n",
      "Last batch average cost:  63.9635\n",
      "Current run time is: 18871.570954084396 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27978.847770690918\n",
      "Last batch average cost:  66.1914\n",
      "Current run time is: 19087.193291187286 \n",
      "\n",
      "-------Running Epoch:16-------\n",
      "Trained 1 batches with current epoch cost: 58.34685134887695\n",
      "Last batch average cost:  58.3469\n",
      "Current run time is: 19302.80007958412 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 7000.029960632324\n",
      "Last batch average cost:  66.0027\n",
      "Current run time is: 19518.173004627228 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13955.421634674072\n",
      "Last batch average cost:  66.0754\n",
      "Current run time is: 19732.37994647026 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20879.60273361206\n",
      "Last batch average cost:  62.8699\n",
      "Current run time is: 19946.00799536705 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27811.44805908203\n",
      "Last batch average cost:  61.3703\n",
      "Current run time is: 20159.923812389374 \n",
      "\n",
      "-------Running Epoch:17-------\n",
      "Trained 1 batches with current epoch cost: 62.756935119628906\n",
      "Last batch average cost:  62.7569\n",
      "Current run time is: 20373.80518722534 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6975.328800201416\n",
      "Last batch average cost:  61.708\n",
      "Current run time is: 20587.677181959152 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13887.98963546753\n",
      "Last batch average cost:  62.7167\n",
      "Current run time is: 20802.12410879135 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20755.766193389893\n",
      "Last batch average cost:  61.7233\n",
      "Current run time is: 21016.05283856392 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27626.831035614014\n",
      "Last batch average cost:  63.0859\n",
      "Current run time is: 21229.783714294434 \n",
      "\n",
      "-------Running Epoch:18-------\n",
      "Trained 1 batches with current epoch cost: 63.727176666259766\n",
      "Last batch average cost:  63.7272\n",
      "Current run time is: 21443.33437728882 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6944.142543792725\n",
      "Last batch average cost:  63.1415\n",
      "Current run time is: 21656.855000019073 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13817.609283447266\n",
      "Last batch average cost:  60.0106\n",
      "Current run time is: 21870.421107292175 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20679.56972885132\n",
      "Last batch average cost:  62.3893\n",
      "Current run time is: 22083.78195476532 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27523.1238822937\n",
      "Last batch average cost:  60.1886\n",
      "Current run time is: 22298.675842523575 \n",
      "\n",
      "-------Running Epoch:19-------\n",
      "Trained 1 batches with current epoch cost: 62.49665069580078\n",
      "Last batch average cost:  62.4967\n",
      "Current run time is: 22512.59840941429 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6875.465686798096\n",
      "Last batch average cost:  60.0354\n",
      "Current run time is: 22725.698153972626 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13750.059425354004\n",
      "Last batch average cost:  61.4924\n",
      "Current run time is: 22939.11530160904 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20594.605010986328\n",
      "Last batch average cost:  59.2757\n",
      "Current run time is: 23152.656027793884 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27392.819023132324\n",
      "Last batch average cost:  62.6984\n",
      "Current run time is: 23366.789667367935 \n",
      "\n",
      "-------Running Epoch:20-------\n",
      "Trained 1 batches with current epoch cost: 62.86162185668945\n",
      "Last batch average cost:  62.8616\n",
      "Current run time is: 23580.557030439377 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6886.882080078125\n",
      "Last batch average cost:  65.23\n",
      "Current run time is: 23794.644724845886 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13687.60287475586\n",
      "Last batch average cost:  62.126\n",
      "Current run time is: 24008.49728679657 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20466.768772125244\n",
      "Last batch average cost:  59.4638\n",
      "Current run time is: 24222.303117513657 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27259.288234710693\n",
      "Last batch average cost:  64.3808\n",
      "Current run time is: 24435.715591192245 \n",
      "\n",
      "-------Running Epoch:21-------\n",
      "Trained 1 batches with current epoch cost: 59.902889251708984\n",
      "Last batch average cost:  59.9029\n",
      "Current run time is: 24650.03305721283 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6811.882591247559\n",
      "Last batch average cost:  62.1706\n",
      "Current run time is: 24864.959515571594 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13575.733139038086\n",
      "Last batch average cost:  63.3802\n",
      "Current run time is: 25082.11634159088 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20332.63257598877\n",
      "Last batch average cost:  62.8928\n",
      "Current run time is: 25298.19077348709 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27105.530422210693\n",
      "Last batch average cost:  66.3033\n",
      "Current run time is: 25511.936408996582 \n",
      "\n",
      "-------Running Epoch:22-------\n",
      "Trained 1 batches with current epoch cost: 62.48832702636719\n",
      "Last batch average cost:  62.4883\n",
      "Current run time is: 25727.217967510223 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6843.519737243652\n",
      "Last batch average cost:  61.5686\n",
      "Current run time is: 25941.105458498 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13610.025390625\n",
      "Last batch average cost:  61.3461\n",
      "Current run time is: 26154.881526470184 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20303.756427764893\n",
      "Last batch average cost:  63.7137\n",
      "Current run time is: 26369.2566075325 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 27022.61446762085\n",
      "Last batch average cost:  61.1118\n",
      "Current run time is: 26583.395114421844 \n",
      "\n",
      "-------Running Epoch:23-------\n",
      "Trained 1 batches with current epoch cost: 62.425960540771484\n",
      "Last batch average cost:  62.426\n",
      "Current run time is: 26798.119177103043 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6800.374431610107\n",
      "Last batch average cost:  61.0428\n",
      "Current run time is: 27012.60832095146 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13501.757339477539\n",
      "Last batch average cost:  57.2792\n",
      "Current run time is: 27227.227919101715 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20228.70079421997\n",
      "Last batch average cost:  60.3265\n",
      "Current run time is: 27441.425930023193 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26981.23204421997\n",
      "Last batch average cost:  62.0478\n",
      "Current run time is: 27655.438297510147 \n",
      "\n",
      "-------Running Epoch:24-------\n",
      "Trained 1 batches with current epoch cost: 60.478057861328125\n",
      "Last batch average cost:  60.4781\n",
      "Current run time is: 27869.268547296524 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6809.602909088135\n",
      "Last batch average cost:  63.0859\n",
      "Current run time is: 28082.98243188858 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13503.442554473877\n",
      "Last batch average cost:  63.0473\n",
      "Current run time is: 28296.581251621246 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20204.450428009033\n",
      "Last batch average cost:  61.7631\n",
      "Current run time is: 28510.373831748962 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26843.14193725586\n",
      "Last batch average cost:  59.7873\n",
      "Current run time is: 28724.743118286133 \n",
      "\n",
      "-------Running Epoch:25-------\n",
      "Trained 1 batches with current epoch cost: 63.238590240478516\n",
      "Last batch average cost:  63.2386\n",
      "Current run time is: 28939.182762384415 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6748.106834411621\n",
      "Last batch average cost:  63.1865\n",
      "Current run time is: 29152.63575077057 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13432.33071899414\n",
      "Last batch average cost:  64.4893\n",
      "Current run time is: 29367.112113952637 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20119.52827835083\n",
      "Last batch average cost:  58.5747\n",
      "Current run time is: 29580.806354045868 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26759.9499168396\n",
      "Last batch average cost:  60.1838\n",
      "Current run time is: 29794.56752228737 \n",
      "\n",
      "-------Running Epoch:26-------\n",
      "Trained 1 batches with current epoch cost: 62.20610427856445\n",
      "Last batch average cost:  62.2061\n",
      "Current run time is: 30008.553996801376 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6744.592720031738\n",
      "Last batch average cost:  57.8572\n",
      "Current run time is: 30222.34151983261 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13385.543716430664\n",
      "Last batch average cost:  60.3961\n",
      "Current run time is: 30435.73014140129 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20043.130168914795\n",
      "Last batch average cost:  57.3676\n",
      "Current run time is: 30649.421882390976 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26695.692302703857\n",
      "Last batch average cost:  55.5361\n",
      "Current run time is: 30863.040069818497 \n",
      "\n",
      "-------Running Epoch:27-------\n",
      "Trained 1 batches with current epoch cost: 61.908382415771484\n",
      "Last batch average cost:  61.9084\n",
      "Current run time is: 31077.312144994736 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6711.05876159668\n",
      "Last batch average cost:  60.4141\n",
      "Current run time is: 31291.79011106491 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13378.607406616211\n",
      "Last batch average cost:  62.9112\n",
      "Current run time is: 31506.15407347679 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 20035.041305541992\n",
      "Last batch average cost:  60.0714\n",
      "Current run time is: 31721.133097410202 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26683.04203414917\n",
      "Last batch average cost:  58.3362\n",
      "Current run time is: 31935.71632361412 \n",
      "\n",
      "-------Running Epoch:28-------\n",
      "Trained 1 batches with current epoch cost: 59.064090728759766\n",
      "Last batch average cost:  59.0641\n",
      "Current run time is: 32150.548677444458 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6721.652938842773\n",
      "Last batch average cost:  60.5175\n",
      "Current run time is: 32365.34015917778 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13339.003078460693\n",
      "Last batch average cost:  56.5822\n",
      "Current run time is: 32579.68652009964 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 19939.005226135254\n",
      "Last batch average cost:  58.4747\n",
      "Current run time is: 32794.281014204025 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26597.59003829956\n",
      "Last batch average cost:  58.3809\n",
      "Current run time is: 33010.119661808014 \n",
      "\n",
      "-------Running Epoch:29-------\n",
      "Trained 1 batches with current epoch cost: 58.825843811035156\n",
      "Last batch average cost:  58.8258\n",
      "Current run time is: 33225.04749417305 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6716.565807342529\n",
      "Last batch average cost:  58.4546\n",
      "Current run time is: 33439.710698604584 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13339.047649383545\n",
      "Last batch average cost:  61.2667\n",
      "Current run time is: 33654.432509183884 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 19963.057243347168\n",
      "Last batch average cost:  59.1655\n",
      "Current run time is: 33868.59247851372 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26574.6149559021\n",
      "Last batch average cost:  62.8831\n",
      "Current run time is: 34083.22287631035 \n",
      "\n",
      "-------Running Epoch:30-------\n",
      "Trained 1 batches with current epoch cost: 61.76541519165039\n",
      "Last batch average cost:  61.7654\n",
      "Current run time is: 34298.177899837494 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 6655.331134796143\n",
      "Last batch average cost:  56.628\n",
      "Current run time is: 34512.91214132309 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 13301.21575164795\n",
      "Last batch average cost:  60.8247\n",
      "Current run time is: 34727.61437320709 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 19902.945709228516\n",
      "Last batch average cost:  61.4249\n",
      "Current run time is: 34942.8103787899 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 26501.9229927063\n",
      "Last batch average cost:  59.2122\n",
      "Current run time is: 35157.486902713776 \n",
      "\n",
      "At final epoch: 30, the is cost 59.98837661743164\n",
      "Total time taken for run : 35484.737892\n"
     ]
    }
   ],
   "source": [
    "#optimize(30,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring model\n",
    "Here the model is restored and the values in the report match the recovered ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cost(rnn_size,epochs):\n",
    "    \n",
    "\n",
    "    cost_train_list = []\n",
    "    cost_test_list =[]\n",
    "    b_size = 1000\n",
    "    num_train = len(mnist.train.labels)\n",
    "    num_test = len(mnist.test.labels)\n",
    "    n_batches = num_train/b_size\n",
    "    count = 0\n",
    "    i = 0\n",
    "    # Comment out here to use whole training set!\n",
    "    num_train = len(mnist.train.labels[:10000,:])    \n",
    "    n_batches = num_train/b_size\n",
    "    \n",
    "    start = time.time()\n",
    "    while i < num_train:\n",
    "        print('Processing batch number {} of {}.'.format(count+1,n_batches))\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + b_size, num_train)\n",
    "        \n",
    "        if j<= num_test:\n",
    "            \n",
    "            # Get the images from the test-set between index i and j.\n",
    "            images_test = mnist.test.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "            # Get the associated labels.\n",
    "            labels_test = mnist.test.labels[i:j, :]\n",
    "\n",
    "            cost_test = sess.run(cost,feed_dict = {x: binarize(images_test), y: labels_test})\n",
    "        \n",
    "            cost_test_list.append(cost_test)\n",
    "        images_train = mnist.train.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels_test = mnist.train.labels[i:j, :]\n",
    "\n",
    "        cost_train = sess.run(cost,feed_dict = {x: binarize(images_train), y: labels_test})\n",
    "        cost_train_list.append(cost_train)\n",
    "        i = j\n",
    "        count +=1\n",
    "    \n",
    "    total_cost_train = sum(cost_train_list)/len(cost_train_list)\n",
    "    total_cost_test = sum(cost_test_list)/len(cost_test_list)\n",
    "    \n",
    "    print(time.time()-start)\n",
    "    \n",
    "    print('The training cost for {} unit GRU many to many model is {} after {} epochs \\n'.format(rnn_size,total_cost_train,epochs))\n",
    "    \n",
    "    print('The test cost for {} unit GRU many to many model is {} after {} epochs \\n'.format(rnn_size,total_cost_test,epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_MDir = 'models/Task2/gru128'\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_3')\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    "saver2restore = tf.train.Saver()\n",
    "saver2restore.restore(sess = sess, save_path= save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 55.0.\n",
      "Processing batch number 2 of 55.0.\n",
      "Processing batch number 3 of 55.0.\n",
      "Processing batch number 4 of 55.0.\n",
      "Processing batch number 5 of 55.0.\n",
      "Processing batch number 6 of 55.0.\n",
      "Processing batch number 7 of 55.0.\n",
      "Processing batch number 8 of 55.0.\n",
      "Processing batch number 9 of 55.0.\n",
      "Processing batch number 10 of 55.0.\n",
      "Processing batch number 11 of 55.0.\n",
      "Processing batch number 12 of 55.0.\n",
      "Processing batch number 13 of 55.0.\n",
      "Processing batch number 14 of 55.0.\n",
      "Processing batch number 15 of 55.0.\n",
      "Processing batch number 16 of 55.0.\n",
      "Processing batch number 17 of 55.0.\n",
      "Processing batch number 18 of 55.0.\n",
      "Processing batch number 19 of 55.0.\n",
      "Processing batch number 20 of 55.0.\n",
      "Processing batch number 21 of 55.0.\n",
      "Processing batch number 22 of 55.0.\n",
      "Processing batch number 23 of 55.0.\n",
      "Processing batch number 24 of 55.0.\n",
      "Processing batch number 25 of 55.0.\n",
      "Processing batch number 26 of 55.0.\n",
      "Processing batch number 27 of 55.0.\n",
      "Processing batch number 28 of 55.0.\n",
      "Processing batch number 29 of 55.0.\n",
      "Processing batch number 30 of 55.0.\n",
      "Processing batch number 31 of 55.0.\n",
      "Processing batch number 32 of 55.0.\n",
      "Processing batch number 33 of 55.0.\n",
      "Processing batch number 34 of 55.0.\n",
      "Processing batch number 35 of 55.0.\n",
      "Processing batch number 36 of 55.0.\n",
      "Processing batch number 37 of 55.0.\n",
      "Processing batch number 38 of 55.0.\n",
      "Processing batch number 39 of 55.0.\n",
      "Processing batch number 40 of 55.0.\n",
      "Processing batch number 41 of 55.0.\n",
      "Processing batch number 42 of 55.0.\n",
      "Processing batch number 43 of 55.0.\n",
      "Processing batch number 44 of 55.0.\n",
      "Processing batch number 45 of 55.0.\n",
      "Processing batch number 46 of 55.0.\n",
      "Processing batch number 47 of 55.0.\n",
      "Processing batch number 48 of 55.0.\n",
      "Processing batch number 49 of 55.0.\n",
      "Processing batch number 50 of 55.0.\n",
      "Processing batch number 51 of 55.0.\n",
      "Processing batch number 52 of 55.0.\n",
      "Processing batch number 53 of 55.0.\n",
      "Processing batch number 54 of 55.0.\n",
      "Processing batch number 55 of 55.0.\n",
      "317.73993849754333\n",
      "The training cost for 128 unit GRU many to many model is 60.24120864868164 after 30 epochs \n",
      "\n",
      "The test cost for 128 unit GRU many to many model is 59.98836708068848 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cost(rnn_size=128, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 10.0.\n",
      "Processing batch number 2 of 10.0.\n",
      "Processing batch number 3 of 10.0.\n",
      "Processing batch number 4 of 10.0.\n",
      "Processing batch number 5 of 10.0.\n",
      "Processing batch number 6 of 10.0.\n",
      "Processing batch number 7 of 10.0.\n",
      "Processing batch number 8 of 10.0.\n",
      "Processing batch number 9 of 10.0.\n",
      "Processing batch number 10 of 10.0.\n",
      "106.58784127235413\n",
      "The training cost for 128 unit GRU many to many model is 60.05317306518555 after 30 epochs \n",
      "\n",
      "The test cost for 128 unit GRU many to many model is 59.98836708068848 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cost(rnn_size=128, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
