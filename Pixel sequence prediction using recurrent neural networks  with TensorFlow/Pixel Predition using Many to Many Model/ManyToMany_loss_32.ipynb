{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Many to Many Models\n",
    "Here we predict the next pixel for each pixel in the images and calculate the cross entropy of each singular pixel against what the ground truth. Then the loss is summed up for every pixel. Note that there is only ground truth matches for the first 783 pixel predictions since the last output will be out of the input comparison range.\n",
    "\n",
    "The GRU cell is used as from task one it appeared to be more stable and give more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the libraries that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Need to load the MNist data to work with\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)\n",
    "# one hot true gives the y labels as vectors with 1's which correspond to the number it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyper parameters that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 10  # number of digits\n",
    "batch_size = 100 \n",
    "chunk_size = 1 # feeding in pixel by pixel\n",
    "n_chunks = 784 # number of pixells\n",
    "rnn_size = 32 # rnn cell size\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Placeholders to store the data. This shape is used to match the tf rnn cell input\n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size],name='InputData')\n",
    "y = tf.placeholder('float',name='LabelData')\n",
    "\n",
    "# The ground truth of the pixels 2 to end to compare against predicted\n",
    "true_pixels = tf.reshape(x, [-1,n_chunks])\n",
    "true_pixels = true_pixels[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([rnn_size,1]))\n",
    "biases=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784, 32)\n",
      "(?, 32)\n"
     ]
    }
   ],
   "source": [
    "# Here the gru cell is defined of specified size\n",
    "gru_cell = tf.nn.rnn_cell.GRUCell(rnn_size)\n",
    "\n",
    "# The ouputs are a tensor of all the ouput states of the pixels\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = gru_cell, inputs = x,dtype=tf.float32)\n",
    "\n",
    "# Checking to make sure of the correct shape\n",
    "print(outputs.get_shape())\n",
    "print(states.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outputs are reshaped in order to have in (batch*784,RNN_size) for easy matrix multiplication.\n",
    "outputs = tf.reshape(outputs, [-1,rnn_size])\n",
    "# linear transformation\n",
    "linear = tf.matmul(outputs,weights) + biases\n",
    "\n",
    "# Need to reshape so that we have (batch_size, num_pixels)\n",
    "pixel_pred = tf.reshape(linear,[-1,n_chunks])\n",
    "\n",
    "# Since the output is a prediction for the next state, the first 783 is taken since that is the number of GT pixels \n",
    "# available for comparisson\n",
    "pixel_pred = pixel_pred[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "# Using the tf function to calculate the Xent to avoid numerical underflow, it applies a sigmoid layer before \n",
    "# calculating the loss.\n",
    "loss =  tf.nn.sigmoid_cross_entropy_with_logits(pixel_pred,true_pixels)\n",
    "\n",
    "# Sum up hte loss from each pixel to get the loss of an image\n",
    "loss = tf.reduce_sum(loss,1)\n",
    "print(loss.get_shape())\n",
    "\n",
    "# Get the mean image cost and optimize over that.\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/Task2/gru32'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir):\n",
    "    os.makedirs(save_MDir)\n",
    "\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_Take_3')\n",
    "#save_model = os.path.join(save_MDir,'best_accuracy_Take_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(images, threshold=0.1):\n",
    "    return (threshold < images).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "#tf.summary.scalar(\"loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "#tf.summary.scalar(\"accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver2 = tf.train.Saver()\n",
    "\n",
    "# Suggested Directory to use\n",
    "save2_MDir = 'models/Task2/gru32/best'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save2_MDir):\n",
    "    os.makedirs(save2_MDir)\n",
    "\n",
    "save_model2 = os.path.join(save2_MDir,'best_accuracy_3')\n",
    "#save_model2 = os.path.join(save2_MDir,'best_accuracy_')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer function\n",
    "Here the main work is done. Each batch is passed through and outputs the cost at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(hm_epochs, start_epoch):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        count = 0\n",
    "        cost_list=[]\n",
    "        start_epoch_t = time.time()\n",
    "        freq_epoch = hm_epochs/hm_epochs\n",
    "        for epoch in range(hm_epochs):\n",
    "            print(\"-------Running Epoch:{}-------\".format(epoch+1+start_epoch))\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            start = time.time()\n",
    "            n_batches = int(mnist.train.num_examples/batch_size)\n",
    "            #n_batches = 10\n",
    "            freq = int(n_batches/5)\n",
    "            for i in range(n_batches):\n",
    "                # Get the batches ready and into the correct form and shape\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                epoch_x = binarize(epoch_x)\n",
    "                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                \n",
    "                epoch_loss += c\n",
    "                # caluclate the test and train cost per batch to get an idea.\n",
    "                if i% freq ==0 or i == (n_batches):\n",
    "                    \n",
    "                    print(\"Trained {} batches with current epoch cost: {}\".format(i+1,epoch_loss))\n",
    "                    print(\"Last batch average cost: \", c)\n",
    "                    print(\"Current run time is: {} \\n\".format(time.time()-start_epoch_t))\n",
    "            \n",
    "            # At the end of each epoch calculate the current test error\n",
    "            if epoch % freq_epoch==0:\n",
    "                e_cost = sess.run(cost,feed_dict = {x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})\n",
    "                cost_list.append(e_cost)\n",
    "                \n",
    "                print('Epoch', start_epoch+epoch+1, 'completed out of:',hm_epochs+start_epoch,', Current Test loss:',e_cost, ', time:', time.time()-start,'\\n')\n",
    "                print(\"Total time taken for current epoch : {:f} \\n\".format(time.time()-start))\n",
    "                if cost_list[count]== min(cost_list):\n",
    "                        saver2.save(sess= sess, save_path = save_model2)\n",
    "                        print(cost_list)\n",
    "                \n",
    "                count = count+1\n",
    "                        \n",
    "        \n",
    "        Final_cost_test = sess.run(cost,feed_dict = {x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})\n",
    "    \n",
    "        \n",
    "        saver.save(sess= sess, save_path = save_model)\n",
    "        \n",
    "        print(\"At final epoch: {}, the is cost {}\".format(start_epoch+epoch+1, Final_cost_test))\n",
    "    print(\"Total time taken for run : {:f}\".format(time.time()-start_epoch_t))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Running Epoch:1-------\n",
      "Trained 1 batches with current epoch cost: 776.2869262695312\n",
      "Last batch average cost:  776.287\n",
      "Current run time is: 0.5226216316223145 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 32971.74446105957\n",
      "Last batch average cost:  198.394\n",
      "Current run time is: 49.73625922203064 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 52845.00247192383\n",
      "Last batch average cost:  153.307\n",
      "Current run time is: 97.7847671508789 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 67661.33962249756\n",
      "Last batch average cost:  125.87\n",
      "Current run time is: 144.47138905525208 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 81262.58744812012\n",
      "Last batch average cost:  122.727\n",
      "Current run time is: 193.018807888031 \n",
      "\n",
      "Epoch 1 completed out of: 30 , Current Test loss: 116.095 , time: 252.21268153190613 \n",
      "\n",
      "Total time taken for current epoch : 252.213183 \n",
      "\n",
      "[116.09544]\n",
      "-------Running Epoch:2-------\n",
      "Trained 1 batches with current epoch cost: 117.39236450195312\n",
      "Last batch average cost:  117.392\n",
      "Current run time is: 253.2579209804535 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 12755.420867919922\n",
      "Last batch average cost:  110.399\n",
      "Current run time is: 300.27670192718506 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 24950.73062133789\n",
      "Last batch average cost:  107.472\n",
      "Current run time is: 346.8257210254669 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 36539.32445526123\n",
      "Last batch average cost:  99.533\n",
      "Current run time is: 393.18178391456604 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 47661.79828643799\n",
      "Last batch average cost:  100.783\n",
      "Current run time is: 439.98053789138794 \n",
      "\n",
      "Epoch 2 completed out of: 30 , Current Test loss: 96.0723 , time: 246.82698583602905 \n",
      "\n",
      "Total time taken for current epoch : 246.827487 \n",
      "\n",
      "[116.09544, 96.072266]\n",
      "-------Running Epoch:3-------\n",
      "Trained 1 batches with current epoch cost: 94.49320220947266\n",
      "Last batch average cost:  94.4932\n",
      "Current run time is: 500.7513909339905 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 10617.921890258789\n",
      "Last batch average cost:  100.442\n",
      "Current run time is: 550.1774425506592 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 20914.27317047119\n",
      "Last batch average cost:  91.1602\n",
      "Current run time is: 597.816312789917 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 31045.406135559082\n",
      "Last batch average cost:  94.8489\n",
      "Current run time is: 645.7872343063354 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 41002.1767578125\n",
      "Last batch average cost:  91.5687\n",
      "Current run time is: 694.0137138366699 \n",
      "\n",
      "Epoch 3 completed out of: 30 , Current Test loss: 88.372 , time: 255.3619306087494 \n",
      "\n",
      "Total time taken for current epoch : 255.362432 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047]\n",
      "-------Running Epoch:4-------\n",
      "Trained 1 batches with current epoch cost: 86.88497161865234\n",
      "Last batch average cost:  86.885\n",
      "Current run time is: 756.640692949295 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 9879.192039489746\n",
      "Last batch average cost:  88.0929\n",
      "Current run time is: 803.7665510177612 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 19584.965545654297\n",
      "Last batch average cost:  92.9255\n",
      "Current run time is: 850.3170590400696 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 29264.563888549805\n",
      "Last batch average cost:  84.7723\n",
      "Current run time is: 897.7884583473206 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 38824.80547332764\n",
      "Last batch average cost:  85.0082\n",
      "Current run time is: 944.9843287467957 \n",
      "\n",
      "Epoch 4 completed out of: 30 , Current Test loss: 85.4255 , time: 251.66446042060852 \n",
      "\n",
      "Total time taken for current epoch : 251.664962 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514]\n",
      "-------Running Epoch:5-------\n",
      "Trained 1 batches with current epoch cost: 85.6865234375\n",
      "Last batch average cost:  85.6865\n",
      "Current run time is: 1009.0122020244598 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 9511.437240600586\n",
      "Last batch average cost:  89.2145\n",
      "Current run time is: 1058.2062141895294 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 18911.81707763672\n",
      "Last batch average cost:  88.6119\n",
      "Current run time is: 1106.772057056427 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 28311.943641662598\n",
      "Last batch average cost:  86.5161\n",
      "Current run time is: 1155.333072900772 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 37618.69221496582\n",
      "Last batch average cost:  87.0021\n",
      "Current run time is: 1203.5265877246857 \n",
      "\n",
      "Epoch 5 completed out of: 30 , Current Test loss: 83.3455 , time: 259.5888729095459 \n",
      "\n",
      "Total time taken for current epoch : 259.589375 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482]\n",
      "-------Running Epoch:6-------\n",
      "Trained 1 batches with current epoch cost: 83.82384490966797\n",
      "Last batch average cost:  83.8238\n",
      "Current run time is: 1269.1818408966064 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 9310.816459655762\n",
      "Last batch average cost:  83.3805\n",
      "Current run time is: 1317.5034308433533 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 18565.088218688965\n",
      "Last batch average cost:  82.625\n",
      "Current run time is: 1368.8132855892181 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 27709.882858276367\n",
      "Last batch average cost:  84.1419\n",
      "Current run time is: 1418.8029990196228 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 36811.15165710449\n",
      "Last batch average cost:  84.2803\n",
      "Current run time is: 1469.070475101471 \n",
      "\n",
      "Epoch 6 completed out of: 30 , Current Test loss: 81.8468 , time: 261.7080125808716 \n",
      "\n",
      "Total time taken for current epoch : 261.708514 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771]\n",
      "-------Running Epoch:7-------\n",
      "Trained 1 batches with current epoch cost: 81.75491333007812\n",
      "Last batch average cost:  81.7549\n",
      "Current run time is: 1531.5112645626068 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 9147.759674072266\n",
      "Last batch average cost:  82.9172\n",
      "Current run time is: 1579.906869649887 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 18195.445419311523\n",
      "Last batch average cost:  77.403\n",
      "Current run time is: 1628.184594631195 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 27215.371688842773\n",
      "Last batch average cost:  80.3157\n",
      "Current run time is: 1676.215191602707 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 36188.97016906738\n",
      "Last batch average cost:  81.7003\n",
      "Current run time is: 1723.2593970298767 \n",
      "\n",
      "Epoch 7 completed out of: 30 , Current Test loss: 80.831 , time: 253.10171627998352 \n",
      "\n",
      "Total time taken for current epoch : 253.102719 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986]\n",
      "-------Running Epoch:8-------\n",
      "Trained 1 batches with current epoch cost: 82.10296630859375\n",
      "Last batch average cost:  82.103\n",
      "Current run time is: 1785.1363816261292 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 9020.418678283691\n",
      "Last batch average cost:  82.0529\n",
      "Current run time is: 1833.4433624744415 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17955.051315307617\n",
      "Last batch average cost:  84.5277\n",
      "Current run time is: 1881.9789242744446 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 26862.318351745605\n",
      "Last batch average cost:  82.0676\n",
      "Current run time is: 1930.1384375095367 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 35701.65605926514\n",
      "Last batch average cost:  77.0884\n",
      "Current run time is: 1981.038372039795 \n",
      "\n",
      "Epoch 8 completed out of: 30 , Current Test loss: 80.0096 , time: 257.0561435222626 \n",
      "\n",
      "Total time taken for current epoch : 257.056144 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552]\n",
      "-------Running Epoch:9-------\n",
      "Trained 1 batches with current epoch cost: 77.10199737548828\n",
      "Last batch average cost:  77.102\n",
      "Current run time is: 2042.8664486408234 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8908.34774017334\n",
      "Last batch average cost:  79.0445\n",
      "Current run time is: 2092.1651051044464 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17768.1146774292\n",
      "Last batch average cost:  77.6701\n",
      "Current run time is: 2140.740128517151 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 26549.070320129395\n",
      "Last batch average cost:  80.0704\n",
      "Current run time is: 2189.236319541931 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 35297.271812438965\n",
      "Last batch average cost:  78.6764\n",
      "Current run time is: 2238.2266747951508 \n",
      "\n",
      "Epoch 9 completed out of: 30 , Current Test loss: 78.9352 , time: 260.3142201900482 \n",
      "\n",
      "Total time taken for current epoch : 260.314721 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226]\n",
      "-------Running Epoch:10-------\n",
      "Trained 1 batches with current epoch cost: 80.73374938964844\n",
      "Last batch average cost:  80.7337\n",
      "Current run time is: 2303.742723464966 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8835.64250946045\n",
      "Last batch average cost:  76.1402\n",
      "Current run time is: 2353.0500633716583 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17563.97373199463\n",
      "Last batch average cost:  79.854\n",
      "Current run time is: 2402.0360374450684 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 26287.336395263672\n",
      "Last batch average cost:  80.5893\n",
      "Current run time is: 2454.0690190792084 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 34973.126205444336\n",
      "Last batch average cost:  73.0508\n",
      "Current run time is: 2508.052844762802 \n",
      "\n",
      "Epoch 10 completed out of: 30 , Current Test loss: 78.5988 , time: 271.19842505455017 \n",
      "\n",
      "Total time taken for current epoch : 271.198926 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846]\n",
      "-------Running Epoch:11-------\n",
      "Trained 1 batches with current epoch cost: 81.94429779052734\n",
      "Last batch average cost:  81.9443\n",
      "Current run time is: 2575.65860414505 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8743.271240234375\n",
      "Last batch average cost:  79.1199\n",
      "Current run time is: 2624.066562652588 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17453.27207183838\n",
      "Last batch average cost:  78.609\n",
      "Current run time is: 2673.5053634643555 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 26078.925521850586\n",
      "Last batch average cost:  80.3036\n",
      "Current run time is: 2721.0465564727783 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 34709.49649810791\n",
      "Last batch average cost:  80.9525\n",
      "Current run time is: 2768.288232564926 \n",
      "\n",
      "Epoch 11 completed out of: 30 , Current Test loss: 77.9982 , time: 253.13031721115112 \n",
      "\n",
      "Total time taken for current epoch : 253.130819 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184]\n",
      "-------Running Epoch:12-------\n",
      "Trained 1 batches with current epoch cost: 82.46755981445312\n",
      "Last batch average cost:  82.4676\n",
      "Current run time is: 2829.377257347107 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8675.690055847168\n",
      "Last batch average cost:  80.5473\n",
      "Current run time is: 2876.7541193962097 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17280.445930480957\n",
      "Last batch average cost:  80.4324\n",
      "Current run time is: 2924.0073442459106 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25907.02783203125\n",
      "Last batch average cost:  79.8115\n",
      "Current run time is: 2970.8823606967926 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 34496.22705078125\n",
      "Last batch average cost:  79.4425\n",
      "Current run time is: 3017.998445034027 \n",
      "\n",
      "Epoch 12 completed out of: 30 , Current Test loss: 77.2247 , time: 249.54962372779846 \n",
      "\n",
      "Total time taken for current epoch : 249.550125 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693]\n",
      "-------Running Epoch:13-------\n",
      "Trained 1 batches with current epoch cost: 77.34950256347656\n",
      "Last batch average cost:  77.3495\n",
      "Current run time is: 3079.7159321308136 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8645.859191894531\n",
      "Last batch average cost:  79.9123\n",
      "Current run time is: 3127.1816277503967 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17150.353141784668\n",
      "Last batch average cost:  75.8178\n",
      "Current run time is: 3174.189331293106 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25684.774505615234\n",
      "Last batch average cost:  73.537\n",
      "Current run time is: 3220.755573272705 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 34239.0043258667\n",
      "Last batch average cost:  79.4856\n",
      "Current run time is: 3267.9438672065735 \n",
      "\n",
      "Epoch 13 completed out of: 30 , Current Test loss: 76.9073 , time: 248.49586582183838 \n",
      "\n",
      "Total time taken for current epoch : 248.496368 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731]\n",
      "-------Running Epoch:14-------\n",
      "Trained 1 batches with current epoch cost: 77.15057373046875\n",
      "Last batch average cost:  77.1506\n",
      "Current run time is: 3328.7907750606537 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8586.031158447266\n",
      "Last batch average cost:  81.5195\n",
      "Current run time is: 3375.8641259670258 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 17073.836906433105\n",
      "Last batch average cost:  78.1517\n",
      "Current run time is: 3422.140444278717 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25572.072067260742\n",
      "Last batch average cost:  76.9942\n",
      "Current run time is: 3469.3720996379852 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 34058.27574920654\n",
      "Last batch average cost:  78.9083\n",
      "Current run time is: 3520.1921334266663 \n",
      "\n",
      "Epoch 14 completed out of: 30 , Current Test loss: 76.1978 , time: 252.79996395111084 \n",
      "\n",
      "Total time taken for current epoch : 252.800466 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783]\n",
      "-------Running Epoch:15-------\n",
      "Trained 1 batches with current epoch cost: 76.0190658569336\n",
      "Last batch average cost:  76.0191\n",
      "Current run time is: 3582.1747393608093 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8507.267456054688\n",
      "Last batch average cost:  72.1325\n",
      "Current run time is: 3630.598963737488 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16959.832664489746\n",
      "Last batch average cost:  76.1567\n",
      "Current run time is: 3677.547970056534 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25405.453704833984\n",
      "Last batch average cost:  77.5864\n",
      "Current run time is: 3724.774570941925 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33856.74378967285\n",
      "Last batch average cost:  77.2339\n",
      "Current run time is: 3772.8422799110413 \n",
      "\n",
      "Epoch 15 completed out of: 30 , Current Test loss: 75.7615 , time: 250.9847309589386 \n",
      "\n",
      "Total time taken for current epoch : 250.985232 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459]\n",
      "-------Running Epoch:16-------\n",
      "Trained 1 batches with current epoch cost: 77.4869613647461\n",
      "Last batch average cost:  77.487\n",
      "Current run time is: 3833.7487530708313 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8480.126678466797\n",
      "Last batch average cost:  75.4312\n",
      "Current run time is: 3881.8962285518646 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16891.325622558594\n",
      "Last batch average cost:  80.1203\n",
      "Current run time is: 3928.6838104724884 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25298.330764770508\n",
      "Last batch average cost:  76.0301\n",
      "Current run time is: 3975.2350203990936 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33697.691467285156\n",
      "Last batch average cost:  76.1691\n",
      "Current run time is: 4022.05229640007 \n",
      "\n",
      "Epoch 16 completed out of: 30 , Current Test loss: 75.3728 , time: 249.49293661117554 \n",
      "\n",
      "Total time taken for current epoch : 249.493438 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803]\n",
      "-------Running Epoch:17-------\n",
      "Trained 1 batches with current epoch cost: 75.69420623779297\n",
      "Last batch average cost:  75.6942\n",
      "Current run time is: 4084.0134012699127 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8470.32095336914\n",
      "Last batch average cost:  76.2354\n",
      "Current run time is: 4131.451565027237 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16822.838760375977\n",
      "Last batch average cost:  75.8208\n",
      "Current run time is: 4178.51381611824 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25203.845092773438\n",
      "Last batch average cost:  72.5458\n",
      "Current run time is: 4226.323958873749 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33551.58720397949\n",
      "Last batch average cost:  74.6782\n",
      "Current run time is: 4273.75999212265 \n",
      "\n",
      "Epoch 17 completed out of: 30 , Current Test loss: 75.0579 , time: 250.34108448028564 \n",
      "\n",
      "Total time taken for current epoch : 250.341586 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884]\n",
      "-------Running Epoch:18-------\n",
      "Trained 1 batches with current epoch cost: 78.0751953125\n",
      "Last batch average cost:  78.0752\n",
      "Current run time is: 4334.924060583115 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8441.268562316895\n",
      "Last batch average cost:  75.221\n",
      "Current run time is: 4382.63925409317 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16736.660675048828\n",
      "Last batch average cost:  73.5782\n",
      "Current run time is: 4429.189472198486 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25038.6157913208\n",
      "Last batch average cost:  76.6427\n",
      "Current run time is: 4475.767444372177 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33360.85777282715\n",
      "Last batch average cost:  74.3686\n",
      "Current run time is: 4522.316892147064 \n",
      "\n",
      "Epoch 18 completed out of: 30 , Current Test loss: 75.65 , time: 1026.2024235725403 \n",
      "\n",
      "Total time taken for current epoch : 1026.203426 \n",
      "\n",
      "-------Running Epoch:19-------\n",
      "Trained 1 batches with current epoch cost: 77.9270248413086\n",
      "Last batch average cost:  77.927\n",
      "Current run time is: 5361.135390281677 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8411.000953674316\n",
      "Last batch average cost:  70.7433\n",
      "Current run time is: 5410.410546302795 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16736.83016204834\n",
      "Last batch average cost:  74.9929\n",
      "Current run time is: 5459.421598911285 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 25057.012199401855\n",
      "Last batch average cost:  74.7085\n",
      "Current run time is: 5507.154114723206 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33289.50366973877\n",
      "Last batch average cost:  74.9867\n",
      "Current run time is: 5554.491712093353 \n",
      "\n",
      "Epoch 19 completed out of: 30 , Current Test loss: 74.5624 , time: 255.53692245483398 \n",
      "\n",
      "Total time taken for current epoch : 255.537424 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401]\n",
      "-------Running Epoch:20-------\n",
      "Trained 1 batches with current epoch cost: 79.03892517089844\n",
      "Last batch average cost:  79.0389\n",
      "Current run time is: 5617.470890522003 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8351.045890808105\n",
      "Last batch average cost:  74.9376\n",
      "Current run time is: 5664.061816930771 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16634.033180236816\n",
      "Last batch average cost:  72.2683\n",
      "Current run time is: 5711.185190916061 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24893.151260375977\n",
      "Last batch average cost:  73.0178\n",
      "Current run time is: 5758.330001592636 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33139.44706726074\n",
      "Last batch average cost:  75.4035\n",
      "Current run time is: 5805.5625994205475 \n",
      "\n",
      "Epoch 20 completed out of: 30 , Current Test loss: 74.4818 , time: 248.74704432487488 \n",
      "\n",
      "Total time taken for current epoch : 248.747546 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796]\n",
      "-------Running Epoch:21-------\n",
      "Trained 1 batches with current epoch cost: 75.6201400756836\n",
      "Last batch average cost:  75.6201\n",
      "Current run time is: 5866.729071378708 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8313.370544433594\n",
      "Last batch average cost:  76.3536\n",
      "Current run time is: 5913.524144887924 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16566.157012939453\n",
      "Last batch average cost:  75.5042\n",
      "Current run time is: 5960.398501396179 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24792.117637634277\n",
      "Last batch average cost:  73.5138\n",
      "Current run time is: 6007.054367780685 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 33019.84814453125\n",
      "Last batch average cost:  74.3652\n",
      "Current run time is: 6053.950311422348 \n",
      "\n",
      "Epoch 21 completed out of: 30 , Current Test loss: 74.0621 , time: 247.78683352470398 \n",
      "\n",
      "Total time taken for current epoch : 247.786834 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796, 74.062065]\n",
      "-------Running Epoch:22-------\n",
      "Trained 1 batches with current epoch cost: 78.14391326904297\n",
      "Last batch average cost:  78.1439\n",
      "Current run time is: 6115.0671646595 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8300.589027404785\n",
      "Last batch average cost:  74.5771\n",
      "Current run time is: 6162.545430421829 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16507.286277770996\n",
      "Last batch average cost:  73.6679\n",
      "Current run time is: 6210.026625394821 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24706.840950012207\n",
      "Last batch average cost:  73.3893\n",
      "Current run time is: 6257.651593446732 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32898.244316101074\n",
      "Last batch average cost:  80.8579\n",
      "Current run time is: 6304.666396379471 \n",
      "\n",
      "Epoch 22 completed out of: 30 , Current Test loss: 74.2165 , time: 250.02757716178894 \n",
      "\n",
      "Total time taken for current epoch : 250.027577 \n",
      "\n",
      "-------Running Epoch:23-------\n",
      "Trained 1 batches with current epoch cost: 76.95075225830078\n",
      "Last batch average cost:  76.9508\n",
      "Current run time is: 6365.117993116379 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8272.390808105469\n",
      "Last batch average cost:  74.27\n",
      "Current run time is: 6412.050972700119 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16435.569862365723\n",
      "Last batch average cost:  74.8794\n",
      "Current run time is: 6458.760417699814 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24635.479301452637\n",
      "Last batch average cost:  75.6952\n",
      "Current run time is: 6505.554344177246 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32801.817321777344\n",
      "Last batch average cost:  69.9101\n",
      "Current run time is: 6552.531237363815 \n",
      "\n",
      "Epoch 23 completed out of: 30 , Current Test loss: 73.4205 , time: 247.80561590194702 \n",
      "\n",
      "Total time taken for current epoch : 247.806117 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796, 74.062065, 74.216522, 73.42054]\n",
      "-------Running Epoch:24-------\n",
      "Trained 1 batches with current epoch cost: 72.42823028564453\n",
      "Last batch average cost:  72.4282\n",
      "Current run time is: 6613.451498746872 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8230.892295837402\n",
      "Last batch average cost:  76.3222\n",
      "Current run time is: 6660.505706548691 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16379.411338806152\n",
      "Last batch average cost:  70.672\n",
      "Current run time is: 6707.668014764786 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24572.46167755127\n",
      "Last batch average cost:  70.9722\n",
      "Current run time is: 6754.933931112289 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32705.859741210938\n",
      "Last batch average cost:  71.5104\n",
      "Current run time is: 6802.343754768372 \n",
      "\n",
      "Epoch 24 completed out of: 30 , Current Test loss: 73.7915 , time: 251.21283340454102 \n",
      "\n",
      "Total time taken for current epoch : 251.212833 \n",
      "\n",
      "-------Running Epoch:25-------\n",
      "Trained 1 batches with current epoch cost: 74.84072875976562\n",
      "Last batch average cost:  74.8407\n",
      "Current run time is: 6864.74210357666 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8213.643730163574\n",
      "Last batch average cost:  75.3612\n",
      "Current run time is: 6912.372512817383 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16336.068824768066\n",
      "Last batch average cost:  75.0547\n",
      "Current run time is: 6959.975609540939 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24445.18913269043\n",
      "Last batch average cost:  74.5491\n",
      "Current run time is: 7007.572375535965 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32633.522506713867\n",
      "Last batch average cost:  75.2447\n",
      "Current run time is: 7054.910370588303 \n",
      "\n",
      "Epoch 25 completed out of: 30 , Current Test loss: 73.5703 , time: 251.18376541137695 \n",
      "\n",
      "Total time taken for current epoch : 251.184267 \n",
      "\n",
      "-------Running Epoch:26-------\n",
      "Trained 1 batches with current epoch cost: 74.23783111572266\n",
      "Last batch average cost:  74.2378\n",
      "Current run time is: 7115.872853279114 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8194.791664123535\n",
      "Last batch average cost:  74.4358\n",
      "Current run time is: 7166.755035161972 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16315.98225402832\n",
      "Last batch average cost:  70.7088\n",
      "Current run time is: 7214.322290420532 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24438.430519104004\n",
      "Last batch average cost:  78.1063\n",
      "Current run time is: 7261.448847055435 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32571.30926513672\n",
      "Last batch average cost:  75.913\n",
      "Current run time is: 7308.6124403476715 \n",
      "\n",
      "Epoch 26 completed out of: 30 , Current Test loss: 72.9728 , time: 253.8328607082367 \n",
      "\n",
      "Total time taken for current epoch : 253.833362 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796, 74.062065, 74.216522, 73.42054, 73.791451, 73.570335, 72.972778]\n",
      "-------Running Epoch:27-------\n",
      "Trained 1 batches with current epoch cost: 75.58812713623047\n",
      "Last batch average cost:  75.5881\n",
      "Current run time is: 7370.308518648148 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8154.601181030273\n",
      "Last batch average cost:  73.0933\n",
      "Current run time is: 7417.961394786835 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16261.24031829834\n",
      "Last batch average cost:  77.7243\n",
      "Current run time is: 7465.7621104717255 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24375.92153930664\n",
      "Last batch average cost:  71.0811\n",
      "Current run time is: 7513.119874715805 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32469.186111450195\n",
      "Last batch average cost:  74.68\n",
      "Current run time is: 7560.291058540344 \n",
      "\n",
      "Epoch 27 completed out of: 30 , Current Test loss: 73.3394 , time: 250.9794201850891 \n",
      "\n",
      "Total time taken for current epoch : 250.979420 \n",
      "\n",
      "-------Running Epoch:28-------\n",
      "Trained 1 batches with current epoch cost: 74.32182312011719\n",
      "Last batch average cost:  74.3218\n",
      "Current run time is: 7621.321184635162 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8152.799362182617\n",
      "Last batch average cost:  72.9678\n",
      "Current run time is: 7669.700402498245 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16229.5693359375\n",
      "Last batch average cost:  76.7625\n",
      "Current run time is: 7717.965656280518 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24333.28911590576\n",
      "Last batch average cost:  76.6774\n",
      "Current run time is: 7765.743176698685 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32411.505836486816\n",
      "Last batch average cost:  77.7051\n",
      "Current run time is: 7812.985867261887 \n",
      "\n",
      "Epoch 28 completed out of: 30 , Current Test loss: 72.7821 , time: 252.9173047542572 \n",
      "\n",
      "Total time taken for current epoch : 252.917806 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796, 74.062065, 74.216522, 73.42054, 73.791451, 73.570335, 72.972778, 73.339439, 72.782059]\n",
      "-------Running Epoch:29-------\n",
      "Trained 1 batches with current epoch cost: 76.37974548339844\n",
      "Last batch average cost:  76.3797\n",
      "Current run time is: 7874.798775434494 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8152.160705566406\n",
      "Last batch average cost:  71.7328\n",
      "Current run time is: 7922.401610374451 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16190.284866333008\n",
      "Last batch average cost:  73.4303\n",
      "Current run time is: 7970.015389680862 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24275.584266662598\n",
      "Last batch average cost:  72.5698\n",
      "Current run time is: 8017.363175868988 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32355.206481933594\n",
      "Last batch average cost:  69.7251\n",
      "Current run time is: 8064.822434663773 \n",
      "\n",
      "Epoch 29 completed out of: 30 , Current Test loss: 72.5569 , time: 251.50936913490295 \n",
      "\n",
      "Total time taken for current epoch : 251.510372 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796, 74.062065, 74.216522, 73.42054, 73.791451, 73.570335, 72.972778, 73.339439, 72.782059, 72.556892]\n",
      "-------Running Epoch:30-------\n",
      "Trained 1 batches with current epoch cost: 73.99626159667969\n",
      "Last batch average cost:  73.9963\n",
      "Current run time is: 8126.850288152695 \n",
      "\n",
      "Trained 111 batches with current epoch cost: 8133.640182495117\n",
      "Last batch average cost:  73.2058\n",
      "Current run time is: 8174.284556150436 \n",
      "\n",
      "Trained 221 batches with current epoch cost: 16165.708000183105\n",
      "Last batch average cost:  72.6201\n",
      "Current run time is: 8221.775738239288 \n",
      "\n",
      "Trained 331 batches with current epoch cost: 24197.447456359863\n",
      "Last batch average cost:  76.273\n",
      "Current run time is: 8269.4223818779 \n",
      "\n",
      "Trained 441 batches with current epoch cost: 32251.547424316406\n",
      "Last batch average cost:  72.8436\n",
      "Current run time is: 8318.92408823967 \n",
      "\n",
      "Epoch 30 completed out of: 30 , Current Test loss: 72.2218 , time: 255.44372177124023 \n",
      "\n",
      "Total time taken for current epoch : 255.444726 \n",
      "\n",
      "[116.09544, 96.072266, 88.372047, 85.425514, 83.345482, 81.846771, 80.830986, 80.009552, 78.935226, 78.598846, 77.998184, 77.224693, 76.90731, 76.19783, 75.761459, 75.372803, 75.057884, 75.650017, 74.562401, 74.481796, 74.062065, 74.216522, 73.42054, 73.791451, 73.570335, 72.972778, 73.339439, 72.782059, 72.556892, 72.221825]\n",
      "At final epoch: 30, the is cost 72.2218246459961\n",
      "Total time taken for run : 8396.623561\n"
     ]
    }
   ],
   "source": [
    "#optimize(30,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restoring model\n",
    "Here the model is restored and the values in the report match the recovered ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cost(rnn_size,epochs):\n",
    "    \n",
    "\n",
    "    cost_train_list = []\n",
    "    cost_test_list =[]\n",
    "    b_size = 1000\n",
    "    num_train = len(mnist.train.labels)\n",
    "    num_test = len(mnist.test.labels)\n",
    "    n_batches = num_train/b_size\n",
    "    count = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Comment out here to use whole training set!\n",
    "    num_train = len(mnist.train.labels[:10000,:])    \n",
    "    n_batches = num_train/b_size\n",
    "    \n",
    "    start = time.time()\n",
    "    while i < num_train:\n",
    "        print('Processing batch number {} of {}.'.format(count+1,n_batches))\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + b_size, num_train)\n",
    "        \n",
    "        if j<= num_test:\n",
    "            \n",
    "            # Get the images from the test-set between index i and j.\n",
    "            images_test = mnist.test.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "            # Get the associated labels.\n",
    "            labels_test = mnist.test.labels[i:j, :]\n",
    "\n",
    "            cost_test = sess.run(cost,feed_dict = {x: binarize(images_test), y: labels_test})\n",
    "        \n",
    "            cost_test_list.append(cost_test)\n",
    "        images_train = mnist.train.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels_test = mnist.train.labels[i:j, :]\n",
    "\n",
    "        cost_train = sess.run(cost,feed_dict = {x: binarize(images_train), y: labels_test})\n",
    "        cost_train_list.append(cost_train)\n",
    "        i = j\n",
    "        count +=1\n",
    "    \n",
    "    total_cost_train = sum(cost_train_list)/len(cost_train_list)\n",
    "    total_cost_test = sum(cost_test_list)/len(cost_test_list)\n",
    "    \n",
    "    print(time.time()-start)\n",
    "    \n",
    "    print('The training cost for {} unit GRU many to many model is {} after {} epochs \\n'.format(rnn_size,total_cost_train,epochs))\n",
    "    \n",
    "    print('The test cost for {} unit GRU many to many model is {} after {} epochs \\n'.format(rnn_size,total_cost_test,epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_MDir = 'models/Task2/gru32'\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_Take_3')\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    "saver2restore = tf.train.Saver()\n",
    "saver2restore.restore(sess = sess, save_path= save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 55.0.\n",
      "Processing batch number 2 of 55.0.\n",
      "Processing batch number 3 of 55.0.\n",
      "Processing batch number 4 of 55.0.\n",
      "Processing batch number 5 of 55.0.\n",
      "Processing batch number 6 of 55.0.\n",
      "Processing batch number 7 of 55.0.\n",
      "Processing batch number 8 of 55.0.\n",
      "Processing batch number 9 of 55.0.\n",
      "Processing batch number 10 of 55.0.\n",
      "Processing batch number 11 of 55.0.\n",
      "Processing batch number 12 of 55.0.\n",
      "Processing batch number 13 of 55.0.\n",
      "Processing batch number 14 of 55.0.\n",
      "Processing batch number 15 of 55.0.\n",
      "Processing batch number 16 of 55.0.\n",
      "Processing batch number 17 of 55.0.\n",
      "Processing batch number 18 of 55.0.\n",
      "Processing batch number 19 of 55.0.\n",
      "Processing batch number 20 of 55.0.\n",
      "Processing batch number 21 of 55.0.\n",
      "Processing batch number 22 of 55.0.\n",
      "Processing batch number 23 of 55.0.\n",
      "Processing batch number 24 of 55.0.\n",
      "Processing batch number 25 of 55.0.\n",
      "Processing batch number 26 of 55.0.\n",
      "Processing batch number 27 of 55.0.\n",
      "Processing batch number 28 of 55.0.\n",
      "Processing batch number 29 of 55.0.\n",
      "Processing batch number 30 of 55.0.\n",
      "Processing batch number 31 of 55.0.\n",
      "Processing batch number 32 of 55.0.\n",
      "Processing batch number 33 of 55.0.\n",
      "Processing batch number 34 of 55.0.\n",
      "Processing batch number 35 of 55.0.\n",
      "Processing batch number 36 of 55.0.\n",
      "Processing batch number 37 of 55.0.\n",
      "Processing batch number 38 of 55.0.\n",
      "Processing batch number 39 of 55.0.\n",
      "Processing batch number 40 of 55.0.\n",
      "Processing batch number 41 of 55.0.\n",
      "Processing batch number 42 of 55.0.\n",
      "Processing batch number 43 of 55.0.\n",
      "Processing batch number 44 of 55.0.\n",
      "Processing batch number 45 of 55.0.\n",
      "Processing batch number 46 of 55.0.\n",
      "Processing batch number 47 of 55.0.\n",
      "Processing batch number 48 of 55.0.\n",
      "Processing batch number 49 of 55.0.\n",
      "Processing batch number 50 of 55.0.\n",
      "Processing batch number 51 of 55.0.\n",
      "Processing batch number 52 of 55.0.\n",
      "Processing batch number 53 of 55.0.\n",
      "Processing batch number 54 of 55.0.\n",
      "Processing batch number 55 of 55.0.\n",
      "81.65518593788147\n",
      "The training cost for 32 unit GRU many to many model is 72.9030009876598 after 30 epochs \n",
      "\n",
      "The test cost for 32 unit GRU many to many model is 72.2218246459961 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cost(rnn_size=32, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 10.0.\n",
      "Processing batch number 2 of 10.0.\n",
      "Processing batch number 3 of 10.0.\n",
      "Processing batch number 4 of 10.0.\n",
      "Processing batch number 5 of 10.0.\n",
      "Processing batch number 6 of 10.0.\n",
      "Processing batch number 7 of 10.0.\n",
      "Processing batch number 8 of 10.0.\n",
      "Processing batch number 9 of 10.0.\n",
      "Processing batch number 10 of 10.0.\n",
      "22.624350547790527\n",
      "The training cost for 32 unit GRU many to many model is 72.58929901123047 after 30 epochs \n",
      "\n",
      "The test cost for 32 unit GRU many to many model is 72.2218246459961 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cost(rnn_size=32, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
