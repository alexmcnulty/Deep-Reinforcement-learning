{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Many to Many Models\n",
    "Here we predict the next pixel for each pixel in the images and calculate the cross entropy of each singular pixel against what the ground truth. Then the loss is summed up for every pixel. Note that there is only ground truth matches for the first 783 pixel predictions since the last output will be out of the input comparison range.\n",
    "\n",
    "The GRU cell is used as from task one it appeared to be more stable and give more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Need to load the MNist data to work with\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)\n",
    "# one hot true gives the y labels as vectors with 1's which correspond to the number it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 10 # number of digits\n",
    "batch_size = 100\n",
    "chunk_size = 1 # feeding in pixel by pixel\n",
    "n_chunks = 784 # number of pixels\n",
    "rnn_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Placeholders to store the data. This shape is used to match the tf rnn cell input\n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size],name='InputData')\n",
    "y = tf.placeholder('float',name='LabelData')\n",
    "\n",
    "# The ground truth of the pixels 2 to end to compare against predicted\n",
    "true_pixels = tf.reshape(x, [-1,n_chunks])\n",
    "true_pixels = true_pixels[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the varibles that will be used in to transform the 32d layer to 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([rnn_size,1]))\n",
    "biases=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784, 64)\n",
      "(?, 64)\n"
     ]
    }
   ],
   "source": [
    "# Here the gru cell is defined of specified size\n",
    "gru_cell = tf.nn.rnn_cell.GRUCell(rnn_size)\n",
    "\n",
    "# The ouputs are a tensor of all the ouput states of the pixels\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = gru_cell, inputs = x,dtype=tf.float32)\n",
    "\n",
    "# Checking to make sure of the correct shape\n",
    "print(outputs.get_shape())\n",
    "print(states.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outputs are reshaped in order to have in (batch*784,RNN_size) for easy matrix multiplication.\n",
    "outputs = tf.reshape(outputs, [-1,rnn_size])\n",
    "\n",
    "# linear transformation\n",
    "linear = tf.matmul(outputs,weights) + biases\n",
    "\n",
    "# Need to reshape so that we have (batch_size, num_pixels)\n",
    "pixel_pred = tf.reshape(linear,[-1,n_chunks])\n",
    "\n",
    "# Since the output is a prediction for the next state, the first 783 is taken since that is the number of GT pixels \n",
    "# available for comparisson\n",
    "pixel_pred = pixel_pred[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the tf function to calculate the Xent to avoid numerical underflow, it applies a sigmoid layer before \n",
    "# calculating the loss.\n",
    "loss =  tf.nn.sigmoid_cross_entropy_with_logits(pixel_pred,true_pixels)\n",
    "\n",
    "# Sum up the loss from each pixel to get the loss of an image\n",
    "loss = tf.reduce_sum(loss,1)\n",
    "\n",
    "# See the sum of the loss rather than the mean\n",
    "loss_test = tf.reduce_sum(loss)\n",
    "\n",
    "# Get the mean image cost and optimize over that.\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/Task2/gru64'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir):\n",
    "    os.makedirs(save_MDir)\n",
    "\n",
    "#save_model = os.path.join(save_MDir,'best_accuracy_Take_2')\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_Take_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(images, threshold=0.1):\n",
    "    return (threshold < images).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver2 = tf.train.Saver()\n",
    "\n",
    "# Suggested Directory to use\n",
    "save2_MDir = 'models/Task2/gru64/best'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save2_MDir):\n",
    "    os.makedirs(save2_MDir)\n",
    "\n",
    "#save_model2 = os.path.join(save2_MDir,'best_accuracy')\n",
    "save_model2 = os.path.join(save2_MDir,'best_accuracy_3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer function\n",
    "Here the main work is done. Each batch is passed through and outputs the cost at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(hm_epochs, start_epoch):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)    \n",
    "        count = 0\n",
    "        cost_list=[]        \n",
    "        start_epoch_t = time.time()\n",
    "        freq_epoch = hm_epochs/5\n",
    "        # For each epoch loop over all batches and optimize the cost and produce the test cost\n",
    "        for epoch in range(hm_epochs):\n",
    "            print(\"-------Running Epoch:{}-------\".format(epoch+1+start_epoch))\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            start = time.time()\n",
    "            n_batches = int(mnist.train.num_examples/batch_size)\n",
    "            #n_batches = 10\n",
    "            # print batch test and train costs.\n",
    "            freq = int(n_batches/2)\n",
    "            \n",
    "            # Loop over all batches here\n",
    "            for i in range(n_batches):\n",
    "                # Get the batches ready and into the correct form and shape\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                epoch_x = binarize(epoch_x)\n",
    "                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
    "\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                \n",
    "                epoch_loss += c\n",
    "                if i% freq ==0 or i == (n_batches):\n",
    "                    \n",
    "                    print(\"Trained {} batches with current epoch cost: {}\".format(i+1,epoch_loss))\n",
    "                    print(\"Last batch average cost: \", c)\n",
    "                    print(\"Current run time is: {} \\n\".format(time.time()-start_epoch_t))\n",
    "                    \n",
    "            if epoch % freq_epoch==0:\n",
    "                e_loss,e_cost = sess.run([loss_test,cost],feed_dict = {x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})\n",
    "                cost_list.append(e_cost)\n",
    "                \n",
    "                print('Epoch', start_epoch+epoch+1, 'completed out of:',hm_epochs+start_epoch,', Current Test total loss is :',e_loss,' with average',e_cost, ', time:', time.time()-start,'\\n')\n",
    "                print(\"Total time taken for current epoch : {:f} \\n\".format(time.time()-start))\n",
    "                # if the min cost is found then save the model to see if it is much smaller than later epochs.\n",
    "                if cost_list[count]== min(cost_list):\n",
    "                        saver2.save(sess= sess, save_path = save_model2)\n",
    "                        print(cost_list)\n",
    "                \n",
    "                count = count+1\n",
    "                        \n",
    "        \n",
    "        Final_cost_test = sess.run(cost,feed_dict = {x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})\n",
    "    \n",
    "        \n",
    "        saver.save(sess= sess, save_path = save_model)\n",
    "        \n",
    "        print(\"At final epoch: {}, the is cost {}\".format(start_epoch+epoch+1, Final_cost_test))\n",
    "    print(\"Total time taken for run : {:f}\".format(time.time()-start_epoch_t))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Running Epoch:1-------\n",
      "Trained 1 batches with current epoch cost: 329.3008728027344\n",
      "Last batch average cost:  329.301\n",
      "Current run time is: 1.022465467453003 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 41639.0659866333\n",
      "Last batch average cost:  116.368\n",
      "Current run time is: 268.37555480003357 \n",
      "\n",
      "Epoch 1 completed out of: 30 , Current Test total loss is : 913941.0  with average 91.3941 , time: 556.3176465034485 \n",
      "\n",
      "Total time taken for current epoch : 556.318649 \n",
      "\n",
      "[91.394104]\n",
      "-------Running Epoch:2-------\n",
      "Trained 1 batches with current epoch cost: 91.65277099609375\n",
      "Last batch average cost:  91.6528\n",
      "Current run time is: 557.9305911064148 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 23872.678588867188\n",
      "Last batch average cost:  82.658\n",
      "Current run time is: 818.9929077625275 \n",
      "\n",
      "-------Running Epoch:3-------\n",
      "Trained 1 batches with current epoch cost: 79.53643798828125\n",
      "Last batch average cost:  79.5364\n",
      "Current run time is: 1081.3007678985596 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 21895.34413909912\n",
      "Last batch average cost:  78.4163\n",
      "Current run time is: 1350.1257269382477 \n",
      "\n",
      "-------Running Epoch:4-------\n",
      "Trained 1 batches with current epoch cost: 78.63838195800781\n",
      "Last batch average cost:  78.6384\n",
      "Current run time is: 1617.2136714458466 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 21113.382095336914\n",
      "Last batch average cost:  74.1535\n",
      "Current run time is: 1886.3797719478607 \n",
      "\n",
      "-------Running Epoch:5-------\n",
      "Trained 1 batches with current epoch cost: 79.7052993774414\n",
      "Last batch average cost:  79.7053\n",
      "Current run time is: 2149.135839700699 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 20743.15811920166\n",
      "Last batch average cost:  75.9646\n",
      "Current run time is: 2410.3754115104675 \n",
      "\n",
      "-------Running Epoch:6-------\n",
      "Trained 1 batches with current epoch cost: 74.93087768554688\n",
      "Last batch average cost:  74.9309\n",
      "Current run time is: 2703.4686324596405 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 20383.217346191406\n",
      "Last batch average cost:  76.407\n",
      "Current run time is: 2994.3701066970825 \n",
      "\n",
      "-------Running Epoch:7-------\n",
      "Trained 1 batches with current epoch cost: 68.80150604248047\n",
      "Last batch average cost:  68.8015\n",
      "Current run time is: 3256.554616689682 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 20107.06918334961\n",
      "Last batch average cost:  71.0637\n",
      "Current run time is: 3517.1042263507843 \n",
      "\n",
      "Epoch 7 completed out of: 30 , Current Test total loss is : 720404.0  with average 72.0404 , time: 548.5728716850281 \n",
      "\n",
      "Total time taken for current epoch : 548.573875 \n",
      "\n",
      "[91.394104, 72.040375]\n",
      "-------Running Epoch:8-------\n",
      "Trained 1 batches with current epoch cost: 75.68136596679688\n",
      "Last batch average cost:  75.6814\n",
      "Current run time is: 3806.1428656578064 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19906.754997253418\n",
      "Last batch average cost:  69.1012\n",
      "Current run time is: 4067.3858404159546 \n",
      "\n",
      "-------Running Epoch:9-------\n",
      "Trained 1 batches with current epoch cost: 73.53915405273438\n",
      "Last batch average cost:  73.5392\n",
      "Current run time is: 4332.585140943527 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19710.294288635254\n",
      "Last batch average cost:  72.535\n",
      "Current run time is: 4597.555708646774 \n",
      "\n",
      "-------Running Epoch:10-------\n",
      "Trained 1 batches with current epoch cost: 70.85381317138672\n",
      "Last batch average cost:  70.8538\n",
      "Current run time is: 4862.34370470047 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19523.181602478027\n",
      "Last batch average cost:  69.2281\n",
      "Current run time is: 5125.142152070999 \n",
      "\n",
      "-------Running Epoch:11-------\n",
      "Trained 1 batches with current epoch cost: 68.5048828125\n",
      "Last batch average cost:  68.5049\n",
      "Current run time is: 5388.239142894745 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19455.98770904541\n",
      "Last batch average cost:  68.098\n",
      "Current run time is: 5651.406617164612 \n",
      "\n",
      "-------Running Epoch:12-------\n",
      "Trained 1 batches with current epoch cost: 68.33259582519531\n",
      "Last batch average cost:  68.3326\n",
      "Current run time is: 5913.596195697784 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19336.56179046631\n",
      "Last batch average cost:  69.3864\n",
      "Current run time is: 6176.854735851288 \n",
      "\n",
      "-------Running Epoch:13-------\n",
      "Trained 1 batches with current epoch cost: 67.14093017578125\n",
      "Last batch average cost:  67.1409\n",
      "Current run time is: 6439.487504720688 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19241.82188796997\n",
      "Last batch average cost:  69.4199\n",
      "Current run time is: 6703.898907661438 \n",
      "\n",
      "Epoch 13 completed out of: 30 , Current Test total loss is : 692301.0  with average 69.2301 , time: 571.4926424026489 \n",
      "\n",
      "Total time taken for current epoch : 571.494648 \n",
      "\n",
      "[91.394104, 72.040375, 69.230072]\n",
      "-------Running Epoch:14-------\n",
      "Trained 1 batches with current epoch cost: 68.25851440429688\n",
      "Last batch average cost:  68.2585\n",
      "Current run time is: 7011.945482254028 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19122.97510910034\n",
      "Last batch average cost:  71.3644\n",
      "Current run time is: 7329.848491668701 \n",
      "\n",
      "-------Running Epoch:15-------\n",
      "Trained 1 batches with current epoch cost: 72.30780792236328\n",
      "Last batch average cost:  72.3078\n",
      "Current run time is: 7644.831301212311 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19096.784160614014\n",
      "Last batch average cost:  70.2654\n",
      "Current run time is: 7906.289628267288 \n",
      "\n",
      "-------Running Epoch:16-------\n",
      "Trained 1 batches with current epoch cost: 68.9558334350586\n",
      "Last batch average cost:  68.9558\n",
      "Current run time is: 8168.2590255737305 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 19005.054496765137\n",
      "Last batch average cost:  65.8335\n",
      "Current run time is: 8429.777821302414 \n",
      "\n",
      "-------Running Epoch:17-------\n",
      "Trained 1 batches with current epoch cost: 69.03630065917969\n",
      "Last batch average cost:  69.0363\n",
      "Current run time is: 8692.065333604813 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18873.844200134277\n",
      "Last batch average cost:  70.6671\n",
      "Current run time is: 8956.316744804382 \n",
      "\n",
      "-------Running Epoch:18-------\n",
      "Trained 1 batches with current epoch cost: 66.32373046875\n",
      "Last batch average cost:  66.3237\n",
      "Current run time is: 9218.031567811966 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18819.562992095947\n",
      "Last batch average cost:  67.0108\n",
      "Current run time is: 9478.552676200867 \n",
      "\n",
      "-------Running Epoch:19-------\n",
      "Trained 1 batches with current epoch cost: 66.34710693359375\n",
      "Last batch average cost:  66.3471\n",
      "Current run time is: 9739.02158999443 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18804.726135253906\n",
      "Last batch average cost:  68.99\n",
      "Current run time is: 9998.807550907135 \n",
      "\n",
      "Epoch 19 completed out of: 30 , Current Test total loss is : 676123.0  with average 67.6123 , time: 546.1151595115662 \n",
      "\n",
      "Total time taken for current epoch : 546.115661 \n",
      "\n",
      "[91.394104, 72.040375, 69.230072, 67.612328]\n",
      "-------Running Epoch:20-------\n",
      "Trained 1 batches with current epoch cost: 68.41873168945312\n",
      "Last batch average cost:  68.4187\n",
      "Current run time is: 10285.852160692215 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18707.107318878174\n",
      "Last batch average cost:  65.6835\n",
      "Current run time is: 10545.663583278656 \n",
      "\n",
      "-------Running Epoch:21-------\n",
      "Trained 1 batches with current epoch cost: 65.15470886230469\n",
      "Last batch average cost:  65.1547\n",
      "Current run time is: 10806.02477478981 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18657.575859069824\n",
      "Last batch average cost:  67.8571\n",
      "Current run time is: 11065.881417512894 \n",
      "\n",
      "-------Running Epoch:22-------\n",
      "Trained 1 batches with current epoch cost: 65.8551254272461\n",
      "Last batch average cost:  65.8551\n",
      "Current run time is: 11325.96362543106 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18540.389266967773\n",
      "Last batch average cost:  65.0911\n",
      "Current run time is: 11585.672852993011 \n",
      "\n",
      "-------Running Epoch:23-------\n",
      "Trained 1 batches with current epoch cost: 66.0949935913086\n",
      "Last batch average cost:  66.095\n",
      "Current run time is: 11845.893387556076 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18574.265312194824\n",
      "Last batch average cost:  67.5569\n",
      "Current run time is: 12106.594112157822 \n",
      "\n",
      "-------Running Epoch:24-------\n",
      "Trained 1 batches with current epoch cost: 67.29237365722656\n",
      "Last batch average cost:  67.2924\n",
      "Current run time is: 12366.273036241531 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18459.418296813965\n",
      "Last batch average cost:  66.3789\n",
      "Current run time is: 12626.322761058807 \n",
      "\n",
      "-------Running Epoch:25-------\n",
      "Trained 1 batches with current epoch cost: 65.27029418945312\n",
      "Last batch average cost:  65.2703\n",
      "Current run time is: 12886.230016231537 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18393.802040100098\n",
      "Last batch average cost:  67.1308\n",
      "Current run time is: 13145.912728071213 \n",
      "\n",
      "Epoch 25 completed out of: 30 , Current Test total loss is : 658649.0  with average 65.8649 , time: 545.8119049072266 \n",
      "\n",
      "Total time taken for current epoch : 545.811905 \n",
      "\n",
      "[91.394104, 72.040375, 69.230072, 67.612328, 65.864876]\n",
      "-------Running Epoch:26-------\n",
      "Trained 1 batches with current epoch cost: 65.00167846679688\n",
      "Last batch average cost:  65.0017\n",
      "Current run time is: 13432.545012950897 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18365.72703552246\n",
      "Last batch average cost:  66.4964\n",
      "Current run time is: 13693.122097492218 \n",
      "\n",
      "-------Running Epoch:27-------\n",
      "Trained 1 batches with current epoch cost: 64.78292083740234\n",
      "Last batch average cost:  64.7829\n",
      "Current run time is: 13952.628239870071 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18272.481491088867\n",
      "Last batch average cost:  68.7013\n",
      "Current run time is: 14212.129648685455 \n",
      "\n",
      "-------Running Epoch:28-------\n",
      "Trained 1 batches with current epoch cost: 66.69855499267578\n",
      "Last batch average cost:  66.6986\n",
      "Current run time is: 14471.975226163864 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18230.42558670044\n",
      "Last batch average cost:  64.0008\n",
      "Current run time is: 14731.767350912094 \n",
      "\n",
      "-------Running Epoch:29-------\n",
      "Trained 1 batches with current epoch cost: 62.3690185546875\n",
      "Last batch average cost:  62.369\n",
      "Current run time is: 14991.812614679337 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18233.13925933838\n",
      "Last batch average cost:  68.9895\n",
      "Current run time is: 15251.727991819382 \n",
      "\n",
      "-------Running Epoch:30-------\n",
      "Trained 1 batches with current epoch cost: 67.14540100097656\n",
      "Last batch average cost:  67.1454\n",
      "Current run time is: 15511.943934440613 \n",
      "\n",
      "Trained 276 batches with current epoch cost: 18125.09808731079\n",
      "Last batch average cost:  66.5161\n",
      "Current run time is: 15772.706003904343 \n",
      "\n",
      "At final epoch: 30, the is cost 65.00533294677734\n",
      "Total time taken for run : 16059.495321\n"
     ]
    }
   ],
   "source": [
    "#optimize(30,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restoring model\n",
    "Here the model is restored and the values in the report match the recovered ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cost(rnn_size,epochs):\n",
    "    \n",
    "\n",
    "    cost_train_list = []\n",
    "    cost_test_list =[]\n",
    "    b_size = 1000\n",
    "    num_train = len(mnist.train.labels)\n",
    "    num_test = len(mnist.test.labels)\n",
    "    n_batches = num_train/b_size\n",
    "    count = 0\n",
    "    i = 0\n",
    "    start = time.time()\n",
    "    num_train = len(mnist.train.labels[:10000,:])    \n",
    "    n_batches = num_train/b_size\n",
    "    \n",
    "    while i < num_train:\n",
    "        print('Processing batch number {} of {}.'.format(count+1,n_batches))\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + b_size, num_train)\n",
    "        \n",
    "        if j<= num_test:\n",
    "            \n",
    "            # Get the images from the test-set between index i and j.\n",
    "            images_test = mnist.test.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "            # Get the associated labels.\n",
    "            labels_test = mnist.test.labels[i:j, :]\n",
    "\n",
    "            cost_test = sess.run(cost,feed_dict = {x: binarize(images_test), y: labels_test})\n",
    "        \n",
    "            cost_test_list.append(cost_test)\n",
    "        images_train = mnist.train.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels_test = mnist.train.labels[i:j, :]\n",
    "\n",
    "        cost_train = sess.run(cost,feed_dict = {x: binarize(images_train), y: labels_test})\n",
    "        cost_train_list.append(cost_train)\n",
    "        i = j\n",
    "        count +=1\n",
    "    \n",
    "    total_cost_train = sum(cost_train_list)/len(cost_train_list)\n",
    "    total_cost_test = sum(cost_test_list)/len(cost_test_list)\n",
    "    \n",
    "    print(time.time()-start)\n",
    "    \n",
    "    print('The training cost for {} unit GRU many to many model is {} after {} epochs \\n'.format(rnn_size,total_cost_train,epochs))\n",
    "    \n",
    "    print('The test cost for {} unit GRU many to many model is {} after {} epochs \\n'.format(rnn_size,total_cost_test,epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_MDir = 'models/Task2/gru64'\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_Take_3')\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    "saver2restore = tf.train.Saver()\n",
    "saver2restore.restore(sess = sess, save_path= save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 55.0.\n",
      "Processing batch number 2 of 55.0.\n",
      "Processing batch number 3 of 55.0.\n",
      "Processing batch number 4 of 55.0.\n",
      "Processing batch number 5 of 55.0.\n",
      "Processing batch number 6 of 55.0.\n",
      "Processing batch number 7 of 55.0.\n",
      "Processing batch number 8 of 55.0.\n",
      "Processing batch number 9 of 55.0.\n",
      "Processing batch number 10 of 55.0.\n",
      "Processing batch number 11 of 55.0.\n",
      "Processing batch number 12 of 55.0.\n",
      "Processing batch number 13 of 55.0.\n",
      "Processing batch number 14 of 55.0.\n",
      "Processing batch number 15 of 55.0.\n",
      "Processing batch number 16 of 55.0.\n",
      "Processing batch number 17 of 55.0.\n",
      "Processing batch number 18 of 55.0.\n",
      "Processing batch number 19 of 55.0.\n",
      "Processing batch number 20 of 55.0.\n",
      "Processing batch number 21 of 55.0.\n",
      "Processing batch number 22 of 55.0.\n",
      "Processing batch number 23 of 55.0.\n",
      "Processing batch number 24 of 55.0.\n",
      "Processing batch number 25 of 55.0.\n",
      "Processing batch number 26 of 55.0.\n",
      "Processing batch number 27 of 55.0.\n",
      "Processing batch number 28 of 55.0.\n",
      "Processing batch number 29 of 55.0.\n",
      "Processing batch number 30 of 55.0.\n",
      "Processing batch number 31 of 55.0.\n",
      "Processing batch number 32 of 55.0.\n",
      "Processing batch number 33 of 55.0.\n",
      "Processing batch number 34 of 55.0.\n",
      "Processing batch number 35 of 55.0.\n",
      "Processing batch number 36 of 55.0.\n",
      "Processing batch number 37 of 55.0.\n",
      "Processing batch number 38 of 55.0.\n",
      "Processing batch number 39 of 55.0.\n",
      "Processing batch number 40 of 55.0.\n",
      "Processing batch number 41 of 55.0.\n",
      "Processing batch number 42 of 55.0.\n",
      "Processing batch number 43 of 55.0.\n",
      "Processing batch number 44 of 55.0.\n",
      "Processing batch number 45 of 55.0.\n",
      "Processing batch number 46 of 55.0.\n",
      "Processing batch number 47 of 55.0.\n",
      "Processing batch number 48 of 55.0.\n",
      "Processing batch number 49 of 55.0.\n",
      "Processing batch number 50 of 55.0.\n",
      "Processing batch number 51 of 55.0.\n",
      "Processing batch number 52 of 55.0.\n",
      "Processing batch number 53 of 55.0.\n",
      "Processing batch number 54 of 55.0.\n",
      "Processing batch number 55 of 55.0.\n",
      "309.8212876319885\n",
      "The training cost for 64 unit GRU many to many model is 65.64130630493165 after 30 epochs \n",
      "\n",
      "The test cost for 64 unit GRU many to many model is 65.0053337097168 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cost(rnn_size=64, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 10.0.\n",
      "Processing batch number 2 of 10.0.\n",
      "Processing batch number 3 of 10.0.\n",
      "Processing batch number 4 of 10.0.\n",
      "Processing batch number 5 of 10.0.\n",
      "Processing batch number 6 of 10.0.\n",
      "Processing batch number 7 of 10.0.\n",
      "Processing batch number 8 of 10.0.\n",
      "Processing batch number 9 of 10.0.\n",
      "Processing batch number 10 of 10.0.\n",
      "45.222567319869995\n",
      "The training cost for 64 unit GRU many to many model is 65.37458572387695 after 30 epochs \n",
      "\n",
      "The test cost for 64 unit GRU many to many model is 65.0053337097168 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cost(rnn_size=64, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
