{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3: One pixel in-painting\n",
    "In this part we need to get the most likely image given both past and futre pixels. In order to do this, all possible outcomes(pixel predictions) are compared and the outcomes of the lowest cost are chossen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pdb\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Need to load the MNist data to work with\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)\n",
    "# one hot true gives the y labels as vectors with 1's which correspond to the number it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_pixel_inpainting =np.load('one_pixel_inpainting.npy')\n",
    "one_pixel_inpainting1 =np.load('one_pixel_inpainting.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ground truth and the images with 0 and 1 as the missing pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 999, 999, 999], dtype=int64),\n",
       " array([125, 126, 127, ..., 685, 686, 687], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Real hacky way of doing it but even when I was slicing, it would still overwrite.\n",
    "pic_mask0 = one_pixel_inpainting[0,:,:]\n",
    "pic_mask1 = one_pixel_inpainting1[0,:,:]\n",
    "\n",
    "pic_GT = one_pixel_inpainting[1,:,:]\n",
    "np.where(pic_mask0==--1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "        299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "        325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "        377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "        390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "        403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "        416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "        429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "        442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "        455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "        481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "        494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "        507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "        520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "        533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "        546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "        559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "        572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "        585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "        598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "        611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "        624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "        637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "        650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
       "        663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
       "        676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
       "        689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
       "        702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
       "        715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
       "        728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
       "        741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n",
       "        754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
       "        767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n",
       "        780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n",
       "        793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n",
       "        806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
       "        819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n",
       "        832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844,\n",
       "        845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857,\n",
       "        858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870,\n",
       "        871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883,\n",
       "        884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n",
       "        897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n",
       "        910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922,\n",
       "        923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935,\n",
       "        936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948,\n",
       "        949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961,\n",
       "        962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974,\n",
       "        975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987,\n",
       "        988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999], dtype=int64),\n",
       " array([404, 294, 433, 375, 516, 354, 431, 433, 350, 294, 266, 547, 513,\n",
       "        402, 524, 323, 352, 321, 321, 407, 319, 377, 459, 465, 291, 436,\n",
       "        348, 436, 519, 376, 326, 458, 351, 436, 465, 380, 438, 347, 408,\n",
       "        352, 460, 438, 374, 576, 266, 523, 321, 352, 408, 402, 349, 486,\n",
       "        575, 488, 574, 240, 372, 433, 288, 346, 405, 349, 347, 407, 269,\n",
       "        260, 375, 381, 323, 401, 489, 436, 349, 322, 349, 353, 186, 458,\n",
       "        318, 486, 406, 434, 462, 240, 295, 296, 343, 513, 374, 549, 520,\n",
       "        372, 325, 237, 401, 408, 488, 494, 460, 430, 300, 239, 433, 181,\n",
       "        405, 413, 266, 460, 433, 433, 544, 356, 433, 465, 484, 234, 293,\n",
       "        463, 320, 407, 483, 373, 374, 548, 510, 547, 548, 379, 321, 298,\n",
       "        212, 462, 467, 377, 295, 432, 409, 432, 383, 493, 403, 326, 436,\n",
       "        268, 407, 347, 377, 318, 406, 514, 460, 403, 154, 487, 346, 431,\n",
       "        433, 326, 319, 321, 457, 498, 347, 376, 518, 351, 520, 380, 353,\n",
       "        325, 323, 516, 432, 352, 514, 435, 464, 438, 406, 435, 263, 377,\n",
       "        296, 407, 432, 377, 404, 406, 262, 513, 430, 322, 489, 630, 408,\n",
       "        374, 545, 292, 428, 321, 323, 321, 404, 438, 352, 266, 292, 377,\n",
       "        401, 460, 270, 462, 238, 464, 572, 463, 330, 408, 405, 370, 298,\n",
       "        435, 487, 463, 543, 405, 406, 488, 493, 630, 432, 324, 296, 437,\n",
       "        355, 407, 400, 212, 433, 349, 292, 432, 323, 376, 404, 492, 347,\n",
       "        323, 410, 377, 377, 295, 462, 321, 434, 325, 463, 373, 186, 297,\n",
       "        323, 490, 522, 459, 436, 374, 464, 463, 409, 407, 374, 520, 402,\n",
       "        404, 491, 405, 488, 290, 380, 430, 322, 350, 382, 434, 325, 462,\n",
       "        547, 462, 518, 351, 289, 549, 516, 319, 404, 463, 466, 378, 433,\n",
       "        245, 322, 321, 404, 208, 436, 434, 431, 459, 405, 460, 432, 348,\n",
       "        410, 406, 601, 434, 264, 431, 265, 346, 405, 349, 405, 462, 349,\n",
       "        357, 459, 457, 463, 458, 263, 408, 350, 261, 493, 432, 218, 202,\n",
       "        372, 266, 436, 521, 373, 264, 435, 408, 407, 376, 300, 286, 488,\n",
       "        352, 316, 462, 353, 462, 295, 378, 513, 357, 433, 432, 378, 462,\n",
       "        411, 541, 431, 432, 572, 406, 327, 347, 462, 459, 320, 294, 379,\n",
       "        379, 319, 290, 439, 457, 458, 203, 435, 441, 299, 384, 516, 401,\n",
       "        237, 404, 296, 407, 295, 324, 488, 324, 290, 571, 432, 434, 344,\n",
       "        375, 377, 351, 351, 297, 350, 462, 237, 546, 320, 411, 492, 409,\n",
       "        349, 434, 431, 320, 576, 296, 405, 348, 493, 483, 318, 269, 298,\n",
       "        295, 402, 485, 241, 379, 357, 574, 461, 432, 295, 320, 408, 155,\n",
       "        376, 489, 323,  97, 549, 575, 409, 318, 464, 431, 386, 320, 267,\n",
       "        379, 403, 412, 437, 602, 375, 350, 408, 241, 435, 347, 294, 438,\n",
       "        347, 543, 319, 207, 459, 266, 320, 323, 323, 378, 294, 517, 525,\n",
       "        348, 317, 488, 437, 382, 517, 432, 321, 511, 518, 383, 460, 262,\n",
       "        351, 440, 181, 265, 295, 495, 435, 319, 237, 433, 578, 295, 318,\n",
       "        321, 351, 462, 432, 351, 327, 436, 349, 407, 687, 292, 353, 545,\n",
       "        265, 436, 318, 351, 410, 291, 236, 436, 522, 458, 319, 293, 379,\n",
       "        262, 348, 401, 486, 378, 297, 401, 512, 378, 434, 597, 432, 483,\n",
       "        406, 325, 406, 495, 512, 375, 350, 435, 403, 348, 460, 431, 210,\n",
       "        289, 268, 573, 292, 484, 434, 518, 406, 406, 239, 238, 403, 377,\n",
       "        489, 409, 298, 292, 271, 348, 238, 515, 212, 404, 407, 435, 261,\n",
       "        407, 627, 457, 321, 349, 353, 264, 518, 461, 397, 402, 376, 406,\n",
       "        269, 357, 460, 434, 236, 431, 404, 289, 466, 261, 463, 320, 546,\n",
       "        605, 378, 358, 206, 659, 518, 431, 370, 517, 461, 217, 347, 351,\n",
       "        327, 381, 406, 402, 298, 293, 322, 462, 181, 295, 321, 513, 490,\n",
       "        517, 492, 378, 237, 517, 517, 263, 325, 269, 486, 407, 353, 374,\n",
       "        602, 378, 404, 494, 380, 456, 516, 409, 402, 376, 436, 290, 265,\n",
       "        436, 517, 351, 486, 380, 403, 428, 375, 407, 471, 547, 319, 265,\n",
       "        512, 212, 209, 350, 295, 346, 460, 489, 487, 376, 349, 550, 382,\n",
       "        460, 460, 376, 547, 293, 487, 460, 404, 434, 547, 401, 485, 349,\n",
       "        522, 432, 491, 354, 462, 351, 436, 377, 519, 292, 490, 404, 494,\n",
       "        374, 184, 376, 412, 265, 373, 409, 347, 351, 433, 351, 263, 462,\n",
       "        544, 436, 460, 343, 299, 434, 383, 407, 322, 573, 436, 328, 294,\n",
       "        435, 462, 349, 431, 379, 374, 432, 522, 461, 351, 460, 351, 461,\n",
       "        378, 294, 376, 431, 461, 465, 296, 355, 459, 293, 351, 354, 430,\n",
       "        295, 492, 350, 464, 462, 406, 381, 321, 297, 401, 464, 523, 345,\n",
       "        326, 348, 434, 205, 319, 516, 439, 490, 434, 266, 350, 403, 545,\n",
       "        465, 377, 375, 408, 429, 351, 463, 351, 351, 382, 290, 377, 408,\n",
       "        383, 290, 380, 320, 521, 377, 239, 350, 429, 492, 293, 382, 375,\n",
       "        379, 238, 544, 209, 266, 459, 435, 438, 410, 547, 372, 295, 350,\n",
       "        320, 518, 487, 320, 377, 489, 406, 407, 323, 293, 435, 465, 375,\n",
       "        438, 516, 486, 410, 435, 433, 374, 607, 428, 319, 464, 375, 350,\n",
       "        323, 428, 434, 294, 380, 435, 295, 407, 236, 514, 489, 516, 492,\n",
       "        461, 577, 378, 465, 214, 465, 324, 383, 412, 386, 295, 406, 435,\n",
       "        410, 522, 547, 321, 351, 433, 433, 379, 544, 410, 429, 434, 292,\n",
       "        460, 437, 460, 407, 459, 268, 462, 316, 232, 515, 376, 320, 435,\n",
       "        290, 352, 459, 350, 382, 350, 377, 375, 268, 575, 433, 488, 293,\n",
       "        486, 430, 181, 265, 263, 458, 267, 436, 405, 411, 407, 324, 321,\n",
       "        315, 292, 375, 379, 404, 295, 353, 348, 325, 321, 239, 432, 234,\n",
       "        406, 460, 436, 459, 236, 297, 432, 298, 295, 384, 240, 408, 525,\n",
       "        374, 436, 570, 631, 517, 212, 434, 514, 409, 322, 494, 403, 380,\n",
       "        373, 321, 518, 324, 492, 488, 395, 271, 325, 265, 267, 489, 376,\n",
       "        398, 462, 509, 382, 402, 349, 324, 463, 433, 548, 402, 432], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pic_mask0==-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the missing pixels indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = pic_mask0 - pic_GT\n",
    "indexes =(diff).nonzero()\n",
    "miss_pix_ind =indexes[1] # missng pixel indeces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace the -1 in the images with either ones or zeros\n",
    "pixel_one = pic_mask1.copy()\n",
    "pixel_zero = pic_mask0.copy()\n",
    "\n",
    "for i in range(len(miss_pix_ind)):\n",
    "    pixel_zero[i,miss_pix_ind[i]] =0\n",
    "    \n",
    "    pixel_one[i,miss_pix_ind[i]] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN_predict(input_pixels,gt_pixels,rnn_size):\n",
    "\n",
    "\n",
    "    probs = []\n",
    "\n",
    "    # GRU layer\n",
    "    gru_cell = tf.nn.rnn_cell.GRUCell(num_units=rnn_size)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=gru_cell, dtype=tf.float32, inputs=input_pixels)\n",
    "    # Shape outputs so its batch size=100*484, rnn_size=32\n",
    "    outputs = tf.reshape(outputs, [-1, rnn_size])\n",
    "    \n",
    "    # affine transformation\n",
    "    outputs = tf.matmul(outputs, weights) + biases\n",
    "    \n",
    "    # Reshape so that (batch size,pix_len)\n",
    "    sampled = tf.reshape(input_pixels,[-1, 784])\n",
    "    gt = tf.reshape(gt_pixels,[-1, 784])\n",
    "    logits = tf.reshape((outputs), [-1, 784])\n",
    "    # Calculate the cost between the ground truth image and the probs(outcomes)\n",
    "    cost_gt = -tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(targets=gt[:,1:], logits=logits[:,:-1]), axis = 1)\n",
    "    \n",
    "    # Calculate the cost between the image you sampled and the probs(outcomes)\n",
    "    cost_sampled = -tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(targets=sampled[:,1:], logits=logits[:,:-1]), axis = 1)\n",
    "    \n",
    "    outputs = tf.reshape(tf.nn.sigmoid(outputs), [-1, 784])\n",
    "    return cost_sampled,cost_gt, outputs, logits\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(images, threshold=0.1):\n",
    "    return (threshold < images).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders, variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "batch_size = 100\n",
    "chunk_size = 1\n",
    "n_chunks = 784\n",
    "rnn_size = 128\n",
    "first_pix = 484\n",
    "#seq_len = 10\n",
    "#\n",
    "#full = first_pix+seq_len\n",
    "sampled_seq_list =[]\n",
    "cost_list=[]\n",
    "cost_values = []\n",
    "n_samples = 10\n",
    "random.seed(10)\n",
    "seq_len = miss_pix_ind\n",
    "\n",
    "# Define the placeholders that will be used\n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size])\n",
    "y = tf.placeholder('float', [None, n_chunks,chunk_size])\n",
    "\n",
    "# Taking the first 484 pixels of the fitst 100 images\n",
    "x_in_paint = x[:,0:first_pix,:]\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([rnn_size, 1]))\n",
    "biases =  tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the graph\n",
    "cost_sampled, cost_gt,probs, logits = RNN_predict(x,y,rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:    \n",
    "    sess.run(init)\n",
    "    save_MDir = 'models/Task2/gru128/'\n",
    "    save_model = os.path.join(save_MDir,'best_accuracy_3')\n",
    "    saver2restore = tf.train.Saver()\n",
    "    saver2restore.restore(sess = sess, save_path= save_model)\n",
    "    start = time.time()\n",
    "    \n",
    "    # Compute the costs and probs for the images with 0 as the missing pixel\n",
    "    batch_x = pixel_zero\n",
    "    batch_x = binarize(batch_x)\n",
    "    batch_x = batch_x.reshape(1000,784,chunk_size)\n",
    "    \n",
    "    \n",
    "    # main work done here\n",
    "    cost_s0,cost_gt0,pix_probs0,logits0 = sess.run([cost_sampled,cost_gt,probs,logits],feed_dict={x:batch_x, y:pic_GT.reshape(1000,784,chunk_size)})\n",
    "    \n",
    "    # Compute the costs and probs for the images with 1 as the missing pixel\n",
    "    batch_y = pixel_one\n",
    "    batch_y = binarize(batch_y)\n",
    "    batch_y = batch_y.reshape(1000,784,chunk_size)\n",
    "    #batch_y = mnist.test.labels[:batch_size]\n",
    "    \n",
    "    # main work done here\n",
    "    cost_s1,cost_gt1,pix_probs1, logits1 = sess.run([cost_sampled,cost_gt,probs,logits],feed_dict={x:batch_y, y:pic_GT.reshape(1000,784,chunk_size)})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stack the cost toger to compare\n",
    "compare=np.stack((cost_s0, cost_s1),axis =1).reshape(1000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# Since took the negative, look at max, not min\n",
    "predictions = np.argmax(compare, axis = 1)\n",
    "accuracy = 0\n",
    "\n",
    "# compare the predicted indices to the actual missing \n",
    "for i  in range(1000):\n",
    "    if pic_GT[i, miss_pix_ind[i]]== predictions[i]:\n",
    "         accuracy+=1\n",
    "print(accuracy/1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_Xent = 0\n",
    "gt_Xent =0\n",
    "pred_image = []\n",
    "accuracy = 0\n",
    "count =0\n",
    "count2 = 0\n",
    "mistake_list=[]\n",
    "# loop over each image check if correct and sum up the sample cost and ml cost.\n",
    "for i in range(1000):\n",
    "    if predictions[i]==0:\n",
    "        ml_Xent += -cost_s0[i]\n",
    "        gt_Xent += -cost_gt0[i]\n",
    "        pred_image.append(pixel_zero[i,:])\n",
    "        accuracy += (0 == pic_GT[i, miss_pix_ind[i]])\n",
    "        count +=1\n",
    "        if 0 != pic_GT[i, miss_pix_ind[i]]:\n",
    "            mistake_list.append(i)\n",
    "    else:\n",
    "        ml_Xent += -cost_s1[i]\n",
    "        gt_Xent += -cost_gt1[i]\n",
    "        pred_image.append(pixel_one[i,:])\n",
    "        accuracy += (1 == pic_GT[i, miss_pix_ind[i]])\n",
    "        count +=1\n",
    "        if 1 != pic_GT[i, miss_pix_ind[i]]:\n",
    "            mistake_list.append(i)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum likelihood average loss is: 61.6105\n",
      "The ground truth average loss is: 61.6393\n",
      "The number of correctly predicted pixels is : 95.0%\n"
     ]
    }
   ],
   "source": [
    "print('The maximum likelihood average loss is: {:.6}'.format(ml_Xent/1000))\n",
    "print('The ground truth average loss is: {:.6}'.format(gt_Xent/1000))\n",
    "print('The number of correctly predicted pixels is : {:.1%}'.format(accuracy/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit_images = np.asarray(pred_image)\n",
    "submit_images =submit_images.reshape(1,1000,784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_inpaintings = np.append(one_pixel_inpainting,submit_images,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.save('one_pixel_inpainting_submit.npy', new_inpaintings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reloaded_model =  np.load('one_pixel_inpainting_submit.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('one_pixel_inpainting.npy', new_inpaintings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reloaded_model =  np.load('one_pixel_inpainting.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAEICAYAAABlM/5GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADPNJREFUeJzt3V2MbWdZB/D/0x5psYUK1DQ0IlWUxBSlVS/ESPGmIuBX\nFBItgtGEiA16Q6JeVCNYvTB6KR/RmkYDMUUqIEmtVO0FQkwMKRf1o0ZsU/sBgVjtOdbSpq8XsyeZ\nHnvO7Nmz1372XvP7JZPO7Fmz9trz7P2f533W6j41xggAfS7oPgCAk04QAzQTxADNBDFAM0EM0EwQ\nAzQTxIeoqquqalTVqe5jYX3UdZ52ta6zCOKqur+qnqiq01X1xaq6taou3dB9v7iq/ryqzlTVA1V1\nwybu9yRoruu7quofqurJqrp1E/d5UnTVtaouqqpbFq/Tx6vqnqp6w9T3u4xZBPHCD48xLk3ynUm+\nO8lNZ29Qe9b9mH8/yVeTXJHkrUneX1VXr/k+TrKuuj6c5OYkf7Tm/bKno66nkjyY5HVJLlvc521V\nddUa72MlcwriJMkY46EkdyR5VZJU1d1V9VtV9XdJ/ifJN1fVZYu/jI9U1UNVdXNVXbjY/sKq+t2q\n+nJVfSHJm851X1V1SZKfSPJrY4zTY4xPJ/l4krdN/DBPnE3WdXF/t48xPpbkK9M+spNtk3UdY5wZ\nY/zGGOP+McYzY4xPJvn3JN81+QM9xE7NUZZRVS9L8sYktx+4+W1J3pDkX5JUktuSfCnJtyS5JMkn\ns/eX8oNJ3pHkh5Jcm+RMko+e5+5emeTpMcZ9B277fJLvX8ND4YAN15UN6axrVV2Rvdfwvcd9HMc2\nxtj5jyT3Jzmd5LEkDyR5X5LnL753d5L3Htj2iiRP7n9/cdtPJfnbxed/k+SdB773A0lGklPPcb+v\nTfLoWbe9I8nd3b+TOXx01fWsY7g5ya3dv4s5fWxJXb8myV1JPtj9+xhjzKoj/rExxl3n+N6DBz5/\nefaK8EhV7d92wYFtrjxr+wfOc5+nk7zwrNsuS/L4MgfMUjrqyvTa6rqYO/9J9s7tvOsIxzyZOQXx\n+Rx8i7kHs/cX9vIxxtPPse0jSV524OtvPM9+70tyqqq+dYzxr4vbXp1tWOqcDFPVlV6T1bX20vyW\n7HXabxxjPHXMY12L2Z2sO8wY45Ekf5Xk96rqhVV1QVW9oqpet9jktiS/VFXfUFUvSvKr59nXmezN\ntt5bVZdU1fcl+ZHs/bVlg9ZZ1ySpqlNVdXGSC5NcWFUX79q1qXOw7romeX+Sb8veVRtPTHfkR3Pi\ngnjh7Umel+Qfk/xnkj9L8tLF9/4gyZ3ZO+n2uTz7JMJzuTHJ87N3MuHDSX5hjKEj7rHOut6U5Ins\nvbB/evH5/7vEio1YS12r6uVJfj7JNUkeXVzHfLqq3jrhsS+lFoNrAJqc1I4YYGsIYoBmghigmSAG\naLbRy3Guv+Atzgwe06ee+UgdvtVmqevxqes8LVtXHTFAM0EM0EwQAzQTxADNBDFAM0EM0EwQAzQT\nxADNvL8qs3fnw/ccus3rr7xmA0fCqg6r4a7XT0cM0EwQAzQzmshyS9d9u74Emruj1PJcP6fGbJqO\nGKDZ7DviVTuko+5PF9Vn3TVeBx328W1jXaeiIwZoJogBms1yNLHu60aX2Z+l6Gate9mqZrtnTjXT\nEQM0E8QAzWY5mjiXVZcyB3/uJJ3JnaM5LWfn6KS+vnTEAM0EMUCzWY4mLD/n6aQuW5k/HTFAM0EM\n0GyWowk4yKhqXuZYTx0xQDNBDNDMaIKtdpwrJea4hJ0jV8PoiAHaCWKAZkYTwNZbdcx01LFH1zhL\nRwzQTBADNDOaYOt0XynhH4id3pRXSuziVRg6YoBmghigmdHEEnZxqcNzO04tjSw2a93/wO9R97PJ\nuuqIAZrNsiNex1+1Xbn+cC6mOEG3qZVMVxfFfOiIAZoJYoBmOzeaOOpyc5ntO5aT+8d1kpeyU5xg\n6bZNx8KzHXytbduJVx0xQDNBDNBsJ0YTUy/31rH/VffhjDus5qhjx20eG+mIAZoJYoBmOzGagCks\nMwpa9X/s2eZl8K47yu92V+qgIwZoJogBms1mNDHFMpP56X5fCrbPNlytpCMGaCaIAZrNZjSxqaXl\nNixj5qLjYnsjCLaRjhigmSAGaDab0cQ6GDv02ZX3BNjnuXI8y9R73c+Dba6ZjhigmSAGaLYTo4kp\nlq3bvExhOx3lOeP5tbwpx1K7UgcdMUAzQQzQbCdGEwftylKD1XW/b4jnWJ9VxxS7XjMdMUAzQQzQ\nbOdGE5Ds/lKUw52kGuuIAZoJYoBmghigmSAGaCaIAZoJYoBmghigmSAGaCaIAZoJYoBmghigmSAG\naCaIAZrVGKP7GABONB0xQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QA\nzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0\nE8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQLNZB3FV\n/WRV/X1VnamqLy0+v7H23FFVpxcfT1XVVw98/YFz7O+Gqnpgsb+PVdWLN/2YWG9dq+qlVfWJqnq4\nqkZVXbX5R0Sy9rq+qao+XVWPVdWjVfWHVfWCjse1lDHGLD+SvDvJF5O8OckLklSSa5N8KMlFZ217\na5KbD9nf1UkeT3JdkkuTfDjJn3Y/zpP2MUFdr0hyY5LXJBlJrup+jCfxY4K63pDkB5N8bZIXJbkj\nyQe6H+e5Pmpx0LNSVZcleTjJ28cYH11i+1uT/McY46bzbPPb2XuR3rD4+hVJ/inJS8YYj6/lwDmv\nKep6YNtTSZ5K8k1jjPuPeagcwZR1PfAzP57kPWOMb1/5QCc019HEa5JclOTja9zn1Uk+v//FGOPf\nkjyZ5JVrvA/Ob4q60m8Tdb0uyb0T7v9Y5hrElyf58hjj6f0bquozi3nRE1V13Qr7vDTJf511239n\nbxnFZkxRV/pNWtequj7JzyT59WMe52TmGsRfSXL5YrmZJBljfO8Y4+sW31vlcZ9O8sKzbrsse3Nj\nNmOKutJvsrpW1fdk73zOm8cY9x37SCcy1yfuZ7M3NvjRNe7z3iSv3v9iMSN+XpKtLe4MTVFX+k1S\n16q6NsknkvzcGOOv17nvdTt1+Ca7Z4zxWFW9J8n7qqqS3JnkTJLvSHLJirv9UJLPVtVrk3wuyW8m\nud2Jus2ZqK6pqouTXLj48qKquniM8b/HPmCWMkVdq+pVSf4yyS+OMf5ibQc7kVkGcZKMMX6nqh5K\n8stJ/jh7hf1Ckl9J8pkV9ndvVb0ze4H8kiR3JfnZ9R0xy1h3XReeOPD5Py/+WysfJEc2QV3fneTr\nk9xSVbcsbntgjHH1Oo533WZ5+RrALpnrjBhgZwhigGaCGKCZIAZottGrJq6/4C3ODB7Tp575yNad\nzVfX41PXeVq2rjpigGaCGKCZIAZoJogBmgligGaCGKCZIAZoJogBms32bTBh350P33PoNq+/8poN\nHAmrOqyGu14/HTFAM0EM0MxoIsstXfft+hJo7o5Sy3P9nBqzaTpigGaz74hX7ZCOuj9dVJ9113gd\ndNjHt411nYqOGKCZIAZoNsvRxLqvG11mf5aim7XuZaua7Z451UxHDNBMEAM0m+Vo4lxWXcoc/LmT\ndCZ3jua0nJ2jk/r60hEDNBPEAM1mOZqw/Jynk7psZf50xADNBDFAs1mOJuAgo6p5mWM9dcQAzQQx\nQDOjCbbaca6UmOMSdo5cDaMjBmgniAGaGU0AW2/VMdNRxx5d4ywdMUAzQQzQzGiCrdN9pYR/IHZ6\nU14psYtXYeiIAZoJYoBmRhNL2MWlDs/tOLU0stisdf8Dv0fdzybrqiMGaDbLjngdf9V25frDuZji\nBN2mVjJdXRTzoSMGaCaIAZrt3GjiqMvNZbbvWE7uH9dJXspOcYKl2zYdC8928LW2bSdedcQAzQQx\nQLOdGE1Mvdxbx/5X3Ycz7rCao44dt3lspCMGaCaIAZrtxGgCprDMKGjV/7Fnm5fBu+4ov9tdqYOO\nGKCZIAZoNpvRxBTLTOan+30p2D7bcLWSjhigmSAGaDab0cSmlpbbsIyZi46L7Y0g2EY6YoBmghig\n2WxGE+tg7NBnV94TYJ/nyvEsU+91Pw+2uWY6YoBmghig2U6MJqZYtm7zMoXtdJTnjOfX8qYcS+1K\nHXTEAM0EMUCznRhNHLQrSw1W1/2+IZ5jfVYdU+x6zXTEAM0EMUCznRtNQLL7S1EOd5JqrCMGaCaI\nAZoJYoBmghigmSAGaCaIAZoJYoBmghigmSAGaCaIAZoJYoBmghigmSAGaFZjjO5jADjRdMQAzQQx\nQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNBMEAM0E8QA\nzQQxQDNBDNBMEAM0E8QAzQQxQDNBDNDs/wBVXfF0QZaEVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23bc2e023c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_images = reloaded_model[2,:,:]\n",
    "gt_images = reloaded_model[1,:,:]\n",
    "masked_images = reloaded_model[0,:,:]\n",
    "f2, axar = plt.subplots(2, 3)\n",
    "\n",
    "#correct images\n",
    "for i in range(3):\n",
    "    axar[0, i].imshow(my_images[i,:].reshape(28,28))\n",
    "    axar[0, i].set_title('Pred {}'.format(i))\n",
    "    axar[0, i].axis('off')\n",
    "\n",
    "    axar[1, i].imshow(gt_images[i,:].reshape(28,28))\n",
    "    axar[1, i].set_title('GT {}'.format(i))\n",
    "    axar[1, i].axis('off')\n",
    "    \n",
    "f2.subplots_adjust(hspace=0.7)\n",
    "#plt.savefig('C:/Users/Alex/.ipython/CW2/models/Task2/images/1pix_good.png', dpi =150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of incorrect predctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAEICAYAAADFrJaoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEI5JREFUeJzt3X/wbHVdx/HnC26A8uMKQgyoeQ1zxrD4YWNYoTZFJGg2\nokNB6OSMRYzaNFT2B5o/yJrKtGlCGMUYGxiFRARn8CYamamNjV2auWYUeu8olx+jA8K93hCGT3/s\nWWZdv9+9+93vnnM+u/t8zOzc/XG+55zd9+77vM7nnN2bUgqStOoO6nsFJKkGNkNJwmYoSYDNUJIA\nm6EkATZDSQJWoBkm2ZakJNnS97povqztcuqrrlU0wyS7kuxPsjfJfUmuSXJER8s+JsnHkuxLsjvJ\nBV0sd1X0XNs3JPn3JI8kuaaLZa6Kvuqa5NAkVzef1YeT7Ejy0nnMu4pm2Hh5KeUI4HTgp4DLxifI\nwLzX+W+B7wHHAxcC70ty8pyXser6qu0e4HLgg3Oerwb6qOsW4BvAi4GtzTKvT7JtszOuqRkCUEq5\nG7gVeB5AktuT/EmSfwW+C/xokq3N1uGeJHcnuTzJwc30Byf5yyTfSvI14Nz1lpXkcOA84C2llL2l\nlM8BHwcuavlprqQua9ss78ZSyk3At9t9Zquty7qWUvaVUt5WStlVSnm8lPIJ4OvA8zf7PKoba0ny\nDOAc4MaRuy8CXgr8NxDgeuB+4NnA4cAnGGwtrgJeD7wMOA3YB3x0wuKeAzxWSrlz5L47gJfM4alo\nTMe1VUf6rGuS4xl8jndu9nlQSun9AuwC9gIPAruBK4AnNY/dDrxjZNrjgUeGjzf3/TrwT831zwAX\njzz2S0ABtqyx3DOBe8fuez1we9+vybJc+qrt2DpcDlzT92uxTJdK6vpDwG3AVfN4TjUlw18tpdy2\nzmPfGLn+TAYvwj1JhvcdNDLNiWPT756wzL3AUWP3bQUenmaFNbU+aqv29VbXZhzy7xmM979hA+u8\nrpqa4SSjP63zDQZbmWNLKY+tMe09wDNGbv/IhPneCWxJ8mOllP9p7juFeURuTaut2qpfrdU1g456\nNYPEeU4p5dFNritQ4QGUAyml3AP8I/DuJEclOSjJSUle3ExyPfCmJE9PcjTwRxPmtY/BOMc7khye\n5OeAX2GwxVHH5llbgCRbkhwGHAwcnOQwz0ns3rzrCrwPeC6Do9n757WeC9cMG68BDgG+AjwA/ANw\nQvPY+4HtDA6EfJnvH9RdyyXAkxgM7l4H/E4pxWTYn3nW9jJgP4MP128013/g9A91Yi51TfJM4LeB\nU4F7m/Mc9ya5cLMrmGYgUpJW2qImQ0maK5uhJGEzlCTAZihJQMfnGZ510KuX4mjNpx6/IQeeanVY\n1+W0anU1GUoSNkNJAmyGkgTYDCUJWJwfapC0Arbv2fHE9bNPPLXTZZsMJQmToaQKjCbCvpgMJQmT\noZbAWqmi6/EmzaaGRDhkMpQkbIZaYNv37Fg3WUx6TPUaTfRd19DdZC2UaT8c7ibXbbyOo/UaXh9O\nM/y37ZqaDCWJJUyGG43VJog6bWb3yJrWa1Ii7JvJUJJYkGTY5iBqV+MRmt409R4faFf9Nlun7Xt2\ntPo5NRlKEhUlw3lv3SdtQfr8MrjWN+k9sFadTISLYdaT4sePKrfNZChJ9JwMPfIr2Hgi1GJYtK9J\nVrObPNTmi+Vu1eKY5X1Q8wdN9XM3WZKoMBlqdayX1KdJeGv97ehpUjWf3LvsFnUPzGQoSfScDPve\nWve9/FV1oOQwj5Nz17vPmven9tfeZChJOGaoji3qeJK61/V7xWQoSaxIMvTIYn+62LpP+9VLdaPr\nr9HNy0o0Qy23RfvQaTb+0rUkdcBkqFZN82szG9niL9r3XbVxfSV9k6EksQLJ0N8urM+86mA9l0cN\n474mQ0liBZKhpDrMmv662gNY2mZYQ+zW/FjPxTPr+YZ9DX+4myxJLHEyHOVAu9SfaRJiDZ9Rk6Ek\nsSLJUFL/akh/k5gMJYklTIYedVwNtacMLR6ToSSxhMlQy8kkqLaZDCUJm6EkATZDSQJshpIEQEop\nfa+DJPXOZChJ2AwlCbAZShJgM5QkwGYoSYDNUJIAm6EkATZDSQJshpIE2AwlCbAZShJgM5QkwGYo\nSYDNUJIAm6EkATZDSQJshpIE2AwlCViBZpjkJUm+2fd6aL6s63Lqs65L3ww3KskFSXYn2ZfkpiTH\n9L1O2pwkJyS5OcmeJCXJtr7XSZuX5Nwkn0vyYJJ7k3wgyZGzzs9mOCLJycBVwEXA8cB3gSt6XSnN\nw+PAJ4Hz+l4RzdVW4HLgROC5wNOAv5h1Zr02wyS7kvxBkv9sktjVSY5PcmuSh5PcluTokelvaLYA\n30ny2aZ5DR87J8lXmr+7O8nvr7PMNzXTPX2Nhy8EbimlfLaUshd4C/DKzWxtVlFtdS2l3FdKuQL4\nUitPeEVUWNfrSimfLKV8t5TyAPB+4GdnfoKllN4uwC7giwxS2NOA+4EvA6cBhwGfAf54ZPrXAUcC\nhwLvBXaMPHYPcGZz/Wjg9Ob6S4BvNtff2sz/uHXW5+PAm8fuexh4fp+v06JdaqvryLy2AAXY1vdr\ntIiXWus6Ms/3Ah+e9fltWbNDdutvSin3AST5F+D+Usp/NLc/BvzCcMJSygeH15O8DXggydZSyneA\nR4EfT3JHGWwlHhhZRpL8FfAC4Oeb6ddyBDD+2EMMCqqNqamump8q65rkLOC1wE/P+sRqGDO8b+T6\n/jVuHwGQ5OAkf5bkriQPMdhKARzb/HsecA6wO8k/J3nhyHyeAvwW8KcHeGH3AkeN3beVQTrUxtRU\nV81PdXVNcgZwHfCqUsqdMzwnoI5mOK0LgFcAv8igQW1r7g9AKeVLpZRXAD8M3ARcP/K3DwAvA/4u\nyaQxhZ3AKcMbSU4CDgFmfoF1QF3UVd3rpK5JTgNuBl5XSvn0ZlZ4kZrhkcAjwLeBJwPvGj6Q5JAk\nFzYR/FEGu7aPj/5xKeV2BgdIbkzygnWWcS3w8iRnJjkceCdwYynFZNieLupKksMYjF0BHNrcVnta\nr2uS5zE4S+CNpZRbNrvCi9QMPwTsBu4GvsJgIHfURcCuJpJfzOCF/D6llE8xGNS9Jcnpazy+s/nb\naxkMDh8OXDLH56Af1HpdG/sZDIMAfLW5rfZ0UddLgeOAq5PsbS47Z13hNEdhJGmlLVIylKTW2Awl\nCZuhJAE2Q0kC6PYbKM/663cvxdGar//upel7HWpiXZfTqtXVZChJ2AwlCbAZShJgM5QkoOMDKJI0\nyV3nX/nE9ZM+cnGnyzYZShImQ0kVGE2EfTEZShImQy2BtVJF1+NNmk0NiXDIZChJ2Ay1wO46/8p1\nk8Wkx1Sv0UTfdQ3dTdZCmfbD4W5y3cbrOFqv4fXhNMN/266pyVCSWMJkuNFYbYKo02Z2j6xpvSYl\nwr6ZDCWJBUmGbQ6idjUeoelNU+/xgXbVb7N1uuv8K1v9nJoMJYmKkuG8t+6TtiB9fhlc65v0Hjj7\nxFMB+N/3nDHV9KrHrCfFjx9VbpvJUJLoORl65Fcw+X3wRM3f09HKaG4W7WuS1ewmD7X5Yj379744\nuHJ+a4vQnMzyPqj5g6b6uZssSVSYDNu0fc+OvldBI9bbPZ4m4a31t6OnSdV8cu+yW9QDWyZDSaLn\nZNj31rrv5a+qAyWHeZycu9591rw/tb/2JkNJYsXGDNW/RR1PUve6fq+YDCWJFUmGHlnsTxdb92m/\neqlujH+NbvhVytpPnF+JZqjlZsOr2/CUtpM+csYBppzMX7qWpA6YDNWqtbbmmxm2WLTvu2rj+kr6\nJkNJYgWSob9dWJ951cF6Lo8axn1NhpLECiRDSXWYNf11tQewtM2whtit+bGei2fWn+3va/jD3WRJ\nYomT4SgH2qX+TJMQa/iMmgwliRVJhpL6V0P6m8RkKEksYTL0qONqqD1laPGYDCWJJUyGWk4mQbXN\nZChJ2AwlCbAZShJgM5QkAFJK6XsdJKl3JkNJwmYoSYDNUJIAm6EkATZDSQJshpIE2AwlCbAZShJg\nM5QkwGYoSYDNUJIAm6EkATZDSQJshpIE2AwlCbAZShJgM5QkwGYoSYDNUJKABWiGSX4tyb8l2Zfk\n/ub6JRm4Ncne5vJoku+N3L5ynfldkGR3M7+bkhzT9XPSfOua5IQkNyfZk6Qk2db9MxLMva7nJvlc\nkgeT3JvkA0mObG3lSynVXoBLgfuAVwFHAgFOA64FDh2b9hrg8gPM72TgYeBFwBHAdcCH+36eq3Zp\noa7HA5cALwQKsK3v57iKlxbqegHwy8CTgaOBW4Er21r/av93vCRbgT3Aa0opH51i+muAb5ZSLpsw\nzbsYfFAuaG6fBPwX8NRSysNzWXFN1EZdR6bdAjwKPKuUsmuTq6oNaLOuI3/zSuDtpZSfmHlFJ6h5\nN/mFwKHAx+c4z5OBO4Y3Sil3AY8Az5njMjRZG3VV/7qo64uAnW3NvOZmeCzwrVLKY8M7kny+GT/Y\nn+RFM8zzCOA7Y/c9xCDSqxtt1FX9a7WuSc4CXgu8dZPrua6am+G3gWObXR8ASik/U0p5SvPYLOu+\nFzhq7L6tDMYR1Y026qr+tVbXJGcwGN9/VSnlzk2v6TpqfuN9gcEu7CvmOM+dwCnDG82Y4SFAay+w\nfkAbdVX/WqlrktOAm4HXlVI+Pc95j9ty4En6UUp5MMnbgSuSBNgO7AN+Ejh8xtleC3whyZnAl4F3\nAjd68KQ7LdWVJIcBBzc3D01yWCnl/za9wppKG3VN8jzgk8AbSym3zG1l11FtMwQopfx5kruBPwQ+\nxODF/RrwZuDzM8xvZ5KLGTTFpwK3Ab85vzXWNOZd18b+ketfbf7NzCupDWuhrpcCxwFXJ7m6uW93\nKeXkeazvuGpPrZGkLtU8ZihJnbEZShI2Q0kCbIaSBHR8NPmsg169FEdrPvX4DR6lHGFdl9Oq1dVk\nKEnYDCUJsBlKEmAzlCSg8q/jSVot2/fseOL62See2umyTYaShMlQUgVGE2FfTIaShMlQS2CtVNH1\neJNmU0MiHDIZShI2Qy2w7Xt2rJssJj2meo0m+q5r6G6yFsq0Hw53k+s2XsfReg2vD6cZ/tt2TU2G\nksQSJsONxmoTRJ02s3tkTes1KRH2zWQoSSxIMmxzELWr8QhNb5p6jw+0q36brdP2PTta/ZyaDCWJ\nipLhvLfuk7YgfX4ZXOszES6nWU+KHz+q3DaToSTRczL0yK9g8vvAmi+uRfuaZDW7yUM1v1jqzizv\nA9872gx3kyWJCpNhmxxwr8t69Zgm4a31t6OnSdV8cu+yW9TPmclQkug5Gfa9te57+avqQMlhHifn\nrnefNe9P7a+9yVCSWLExQ/VvUceT1L2u3ysmQ0liRZKhRxb708XWfdqvXqob41+jW5TP20o0Qy03\nG17d5tUU/aVrSeqAyVCtWmtrvplhi0X7vqs2rq+kbzKUJFYgGfrbhfWZVx2s5/KoYdzXZChJrEAy\nlFSHWdNfV3sAS9sMa4jdmh/ruXhm/dn+voY/3E2WJJY4GY5yoF3qzzQJsYbPqMlQkliRZCipfzWk\nv0lMhpLEEiZDjzquhtpThhaPyVCSWMJkqOVkElTbTIaShM1QkgCboSQBNkNJAiCllL7XQZJ6ZzKU\nJGyGkgTYDCUJsBlKEmAzlCTAZihJgM1QkgCboSQBNkNJAmyGkgTYDCUJsBlKEmAzlCTAZihJgM1Q\nkgCboSQBNkNJAmyGkgTYDCUJsBlKEmAzlCTAZihJgM1QkgD4f5k7Zb+LYYZHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b81174eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f2, axar = plt.subplots(3, 3)\n",
    "#mistakes\n",
    "for i in range(3):\n",
    "    axar[0, i].imshow(my_images[mistake_list[i],:].reshape(28,28))\n",
    "    axar[0, i].set_title('Pred {}'.format(i))\n",
    "    axar[0, i].axis('off')\n",
    "    \n",
    "    axar[1, i].imshow(masked_images[mistake_list[i],:].reshape(28,28))\n",
    "    axar[1, i].set_title('mask {}'.format(i))\n",
    "    axar[1, i].axis('off')    \n",
    "    \n",
    "    \n",
    "    axar[2, i].imshow(gt_images[mistake_list[i],:].reshape(28,28))\n",
    "    axar[2, i].set_title('GT {}'.format(i))\n",
    "    axar[2, i].axis('off')\n",
    "    \n",
    "f2.subplots_adjust(hspace=0.7)\n",
    "#plt.savefig('C:/Users/Alex/.ipython/CW2/models/Task2/images/1pix_mistake.png', dpi =150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
