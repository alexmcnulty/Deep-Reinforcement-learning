{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1: Many to one Model:LSTM32\n",
    "In this task we classifiy the MNSIT data set by using a RNN of different sizes and differnt cells. The binarized images are passed into the RNN cell one pixel at a time and the final output is taken. Here the output is transformed into fully connected layer of 100 units with a ReLU activation function. It \n",
    "is then passed through another linear layer of width 10 and a softmax operation is used to get the probabilites of each digit. Cross entropy loss function is used for the cost and the Adam optimizer is used to minimize this value.\n",
    "\n",
    "It was noted that the lstm cell was quite unstable and did not perform as well as the GRU cell over the same number of epochs. Many different learning rates turned out to be too small to learn anything in time or to large such that it would quite well for some time till it jumped to much and lost all accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Need to load the MNist data to work with\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)\n",
    "# one hot true gives the y labels as vectors with 1's which correspond to the number it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the data that we will be working with:\n",
      "train set: 55000 \n",
      "valid set: 5000 \n",
      "test set: 10000 \n"
     ]
    }
   ],
   "source": [
    "print('The size of the data that we will be working with:')\n",
    "print('train set: {} '.format(len(mnist.train.labels)))\n",
    "print('valid set: {} '.format(len(mnist.validation.labels)))\n",
    "print('test set: {} '.format(len(mnist.test.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "n_classes = 10 # digits\n",
    "batch_size = 128\n",
    "chunk_size = 1 # input per timestep\n",
    "n_chunks = 784 # number of pixels/timesteps\n",
    "rnn_size = 32\n",
    "units_output = 100 # output after rnn\n",
    "learning_rate = 0.001\n",
    "\n",
    "# placeholders tp store the inputs and labels \n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size],name='InputData')\n",
    "y = tf.placeholder('float',name='LabelData')\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the varibles that will be used in to transform the 32d layer to 100d fully connected layer.\n",
    "\n",
    "Then for the second fully connected layer from 100 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer1 = {'weights':tf.Variable(tf.random_normal([rnn_size,units_output]),name='Weights1'),\n",
    "             'biases':tf.Variable(tf.random_normal([units_output]),name='Bias')}\n",
    "layer2 = {'weights':tf.Variable(tf.random_normal([units_output,n_classes]),name='Weights2'),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]),name='Bias')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784, 32)\n"
     ]
    }
   ],
   "source": [
    "# Here the lstm cell is defined of specified size\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size,state_is_tuple=True)\n",
    "\n",
    "# The ouputs are a tensor of all the ouput states of the pixels\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = lstm_cell, inputs = x,dtype=tf.float32)\n",
    "\n",
    "# Checking to make sure of the correct shape\n",
    "print(outputs.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Many to one model so we need only the last output of the rnn.\n",
    "outputs = outputs[:, -1, :]\n",
    "\n",
    "# linear transformation\n",
    "output_rnn = tf.matmul(outputs,layer1['weights']) + layer1['biases']\n",
    "\n",
    "# Relu activation\n",
    "act = tf.nn.relu(output_rnn)\n",
    "\n",
    "# linear transformatino\n",
    "output = tf.matmul(act,layer2['weights'])+layer2['biases']\n",
    "\n",
    "# calculate cost of batch\n",
    "Xent =  tf.nn.softmax_cross_entropy_with_logits(output,y)\n",
    "\n",
    "# calculate the average cost per image and optimize\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean( Xent )\n",
    "with tf.name_scope('Adam'):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a boolean of correct labels and take the average to \n",
    "# get the percentage of correctly available\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_label = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_label, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/lstm32/'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir):\n",
    "    os.makedirs(save_MDir)\n",
    "\n",
    "#save_model = os.path.join(save_MDir,'best_accuracyalt_Final')\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(images, threshold=0.1):\n",
    "    return (threshold < images).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Starting new function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "saver2 = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "\n",
    "# Suggested Directory to use\n",
    "save_MDir2 = 'models/lstm32/best'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir2):\n",
    "    os.makedirs(save_MDir2)\n",
    "\n",
    "#save_model = os.path.join(save_MDir,'best_accuracyalt_Final')\n",
    "save_model2 = os.path.join(save_MDir2,'best_accuracy_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(hm_epochs,epoch):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        acc_list=[]\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())        \n",
    "        start_epoch = time.time()\n",
    "        freq_epoch = 1\n",
    "        # For each epoch loop over all batches and optimize the cost and produce the test cost\n",
    "        for epoch in range(hm_epochs):\n",
    "            print(\"-------Running Epoch:{}-------\".format(epoch+1))\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            start = time.time()\n",
    "            n_batches = int(mnist.train.num_examples/batch_size)\n",
    "            freq = int(n_batches/5)\n",
    "            \n",
    "            # print batch test and train costs.                        \n",
    "            for i in range(n_batches):\n",
    "                # Get the batches ready and into the correct form and shape                \n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                epoch_x = binarize(epoch_x)\n",
    "                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
    "\n",
    "\n",
    "                _, c,summary = sess.run([optimizer, cost,merged_summary_op], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                summary_writer.add_summary(summary, epoch * n_batches + i)\n",
    "                epoch_loss += c\n",
    "                # 5 times per epoch get a rough idea of how the train and test accuracies perform on the test and train\n",
    "                # batches.                \n",
    "                if i% freq ==0 or i == (n_batches):\n",
    "                    print(\"Trained {} batches with current epoch cost: {}\".format(i+1,epoch_loss))\n",
    "                    acc_train = sess.run(accuracy,feed_dict = {x: epoch_x, y: epoch_y})\n",
    "                    acc_test = accuracy.eval({x: binarize(mnist.test.images[0:batch_size].reshape((-1, 784, 1))), y: mnist.test.labels[0:batch_size]})\n",
    "                    print(\"At batch: {0}, the training accuracy is: {1:.1%}\".format(i+1, acc_train))\n",
    "                    print(\"At batch: {0}, the test accuracy is: {1:.1%}\".format(i+1, acc_test))\n",
    "                    print(\"Current run time for this batch is: {} \\n\".format(time.time()-start_epoch))\n",
    "            # At the end of the epoch calculate the accuracy                    \n",
    "            if epoch % freq_epoch==0:\n",
    "                print('Epoch', epoch+1, 'completed out of:',hm_epochs,'loss:',epoch_loss, ', time:', time.time()-start,'\\n')\n",
    "                acc_test = accuracy.eval({x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})\n",
    "                print(\"At end of epoch: {0}, the training accuracy in batch is: {1:.1%}\".format(epoch+1, acc_train))\n",
    "                print(\"At end of epoch: {}, the test accuracy is: {:.1%}\".format(epoch+1, acc_test))\n",
    "                acc_list.append(acc_test)\n",
    "                \n",
    "                # If the accuracy is good save model\n",
    "                if epoch>=0:\n",
    "                    if acc_list[epoch]== max(acc_list):\n",
    "                        saver2.save(sess= sess, save_path = save_model2)\n",
    "                        print(acc_list)\n",
    "                        \n",
    "                print(\"Total time taken for current epoch : {:f} \\n\".format(time.time()-start))\n",
    "        \n",
    "        \n",
    "        Final_acc_test,Final_cost_test = sess.run([accuracy,cost],feed_dict = {x: binarize(mnist.test.images.reshape((-1, 784, 1))), y: mnist.test.labels})        \n",
    "        saver.save(sess= sess, save_path = save_model)\n",
    "        print(\"At final epoch: {}, the test accuracy is: {:.1%}, with cost {}\".format(epoch+1, Final_acc_test, Final_cost_test))\n",
    "    print(\"Total time taken for run : {:f}\".format(time.time()-start_epoch))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#optimize(30,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restoring model\n",
    "Here the model is restored and the values in the report match the recovered ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_acc(rnn_size,epochs):\n",
    "    \n",
    "    acc_test_list = []\n",
    "    acc_train_list = []\n",
    "    cost_train_list = []\n",
    "    cost_test_list =[]\n",
    "    b_size = 1000\n",
    "    num_train = len(mnist.train.labels)\n",
    "    num_test = len(mnist.test.labels)\n",
    "    \n",
    "    # Comment out this line line to do over all data set\n",
    "    num_train = len(mnist.train.labels[:10000,:])\n",
    "    n_batches = num_train/b_size\n",
    "    count = 0\n",
    "    i = 0\n",
    "def print_acc(rnn_size,epochs):\n",
    "    \n",
    "    acc_test_list = []\n",
    "    acc_train_list = []\n",
    "    cost_train_list = []\n",
    "    cost_test_list =[]\n",
    "    b_size = 1000\n",
    "    num_train = len(mnist.train.labels)\n",
    "    num_test = len(mnist.test.labels)\n",
    "    \n",
    "    # Comment out here to use whole training set!\n",
    "    num_train = len(mnist.train.labels[:10000,:])\n",
    "    n_batches = num_train/b_size\n",
    "    count = 0\n",
    "    i = 0\n",
    "    start = time.time()\n",
    "    while i < num_train:\n",
    "        print('Processing batch number {} of {}.'.format(count+1,n_batches))\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + b_size, num_train)\n",
    "        \n",
    "        if j<= num_test:\n",
    "            \n",
    "            # Get the images from the test-set between index i and j.\n",
    "            images_test = mnist.test.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "            # Get the associated labels.\n",
    "            labels_test = mnist.test.labels[i:j, :]\n",
    "\n",
    "            acc_test, cost_test = sess.run([accuracy,cost],feed_dict = {x: binarize(images_test), y: labels_test})\n",
    "            #print(cost_test)\n",
    "\n",
    "            acc_test_list.append(acc_test)\n",
    "            cost_test_list.append(cost_test)\n",
    "        images_train = mnist.train.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels_test = mnist.train.labels[i:j, :]\n",
    "\n",
    "        acc_train,cost_train = sess.run([accuracy,cost],feed_dict = {x: binarize(images_train), y: labels_test})\n",
    "        acc_train_list.append(acc_train)\n",
    "        cost_train_list.append(cost_train)\n",
    "        i = j\n",
    "        count +=1\n",
    "        \n",
    "    #print(cost)\n",
    "    #print(time.time()-start)\n",
    "    print('\\n')\n",
    "    \n",
    "    total_acc_train = sum(acc_train_list)/len(acc_train_list)\n",
    "    total_acc_test = sum(acc_test_list)/len(acc_test_list)\n",
    "    total_cost_train = sum(cost_train_list)/len(cost_train_list)\n",
    "    total_cost_test = sum(cost_test_list)/len(cost_test_list)\n",
    "    #print(total_acc)\n",
    "    #total_cost = sum(cost)/len(cost)\n",
    "    print(time.time()-start)\n",
    "    print('The training accuracy for the 3 layer {} unit GRU model is {:.1%} after {} epochs'.format(rnn_size,total_acc_train,epochs))\n",
    "    print('The training cost for the 3 layer {} unit GRU model is {} after {} epochs \\n'.format(rnn_size,total_cost_train,epochs))\n",
    "    print('The test accuracy for the 3 layer {} unit GRU model is {:.1%} after {} epochs'.format(rnn_size,total_acc_test,epochs))\n",
    "    print('The test cost for the 3 layer {} unit GRU model is {} after {} epochs \\n'.format(rnn_size,total_cost_test,epochs))\n",
    "    return(total_acc_train,total_acc_test)    \n",
    "    start = time.time()\n",
    "    while i < num_train:\n",
    "        print('Processing batch number {} of {}.'.format(count+1,n_batches))\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + b_size, num_train)\n",
    "        \n",
    "        if j<= num_test:\n",
    "            \n",
    "            # Get the images from the test-set between index i and j.\n",
    "            images_test = mnist.test.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "            # Get the associated labels.\n",
    "            labels_test = mnist.test.labels[i:j, :]\n",
    "\n",
    "            acc_test, cost_test = sess.run([accuracy,cost],feed_dict = {x: binarize(images_test), y: labels_test})\n",
    "            #print(cost_test)\n",
    "\n",
    "            acc_test_list.append(acc_test)\n",
    "            cost_test_list.append(cost_test)\n",
    "        images_train = mnist.train.images.reshape((-1, 784, 1))[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels_test = mnist.train.labels[i:j, :]\n",
    "\n",
    "        acc_train,cost_train = sess.run([accuracy,cost],feed_dict = {x: binarize(images_train), y: labels_test})\n",
    "        acc_train_list.append(acc_train)\n",
    "        cost_train_list.append(cost_train)\n",
    "        i = j\n",
    "        count +=1\n",
    "        \n",
    "    #print(cost)\n",
    "    #print(time.time()-start)\n",
    "    print('\\n')\n",
    "    \n",
    "    total_acc_train = sum(acc_train_list)/len(acc_train_list)\n",
    "    total_acc_test = sum(acc_test_list)/len(acc_test_list)\n",
    "    total_cost_train = sum(cost_train_list)/len(cost_train_list)\n",
    "    total_cost_test = sum(cost_test_list)/len(cost_test_list)\n",
    "    #print(total_acc)\n",
    "    #total_cost = sum(cost)/len(cost)\n",
    "    print(time.time()-start)\n",
    "    print('The training accuracy for {} unit lstm model is {:.1%} after {} epochs'.format(rnn_size,total_acc_train,epochs))\n",
    "    print('The training cost for {} unit lstm model is {} after {} epochs \\n'.format(rnn_size,total_cost_train,epochs))\n",
    "    print('The test accuracy for {} unit lstm model is {:.1%} after {} epochs'.format(rnn_size,total_acc_test,epochs))\n",
    "    print('The test cost for {} unit lstm model is {} after {} epochs \\n'.format(rnn_size,total_cost_test,epochs))\n",
    "    return(total_acc_train,total_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_MDir = 'models/lstm32/'\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_3')\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    "saver2restore = tf.train.Saver()\n",
    "saver2restore.restore(sess = sess, save_path= save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 55.0.\n",
      "Processing batch number 2 of 55.0.\n",
      "Processing batch number 3 of 55.0.\n",
      "Processing batch number 4 of 55.0.\n",
      "Processing batch number 5 of 55.0.\n",
      "Processing batch number 6 of 55.0.\n",
      "Processing batch number 7 of 55.0.\n",
      "Processing batch number 8 of 55.0.\n",
      "Processing batch number 9 of 55.0.\n",
      "Processing batch number 10 of 55.0.\n",
      "Processing batch number 11 of 55.0.\n",
      "Processing batch number 12 of 55.0.\n",
      "Processing batch number 13 of 55.0.\n",
      "Processing batch number 14 of 55.0.\n",
      "Processing batch number 15 of 55.0.\n",
      "Processing batch number 16 of 55.0.\n",
      "Processing batch number 17 of 55.0.\n",
      "Processing batch number 18 of 55.0.\n",
      "Processing batch number 19 of 55.0.\n",
      "Processing batch number 20 of 55.0.\n",
      "Processing batch number 21 of 55.0.\n",
      "Processing batch number 22 of 55.0.\n",
      "Processing batch number 23 of 55.0.\n",
      "Processing batch number 24 of 55.0.\n",
      "Processing batch number 25 of 55.0.\n",
      "Processing batch number 26 of 55.0.\n",
      "Processing batch number 27 of 55.0.\n",
      "Processing batch number 28 of 55.0.\n",
      "Processing batch number 29 of 55.0.\n",
      "Processing batch number 30 of 55.0.\n",
      "Processing batch number 31 of 55.0.\n",
      "Processing batch number 32 of 55.0.\n",
      "Processing batch number 33 of 55.0.\n",
      "Processing batch number 34 of 55.0.\n",
      "Processing batch number 35 of 55.0.\n",
      "Processing batch number 36 of 55.0.\n",
      "Processing batch number 37 of 55.0.\n",
      "Processing batch number 38 of 55.0.\n",
      "Processing batch number 39 of 55.0.\n",
      "Processing batch number 40 of 55.0.\n",
      "Processing batch number 41 of 55.0.\n",
      "Processing batch number 42 of 55.0.\n",
      "Processing batch number 43 of 55.0.\n",
      "Processing batch number 44 of 55.0.\n",
      "Processing batch number 45 of 55.0.\n",
      "Processing batch number 46 of 55.0.\n",
      "Processing batch number 47 of 55.0.\n",
      "Processing batch number 48 of 55.0.\n",
      "Processing batch number 49 of 55.0.\n",
      "Processing batch number 50 of 55.0.\n",
      "Processing batch number 51 of 55.0.\n",
      "Processing batch number 52 of 55.0.\n",
      "Processing batch number 53 of 55.0.\n",
      "Processing batch number 54 of 55.0.\n",
      "Processing batch number 55 of 55.0.\n",
      "\n",
      "\n",
      "74.08615064620972\n",
      "The training accuracy for 32 unit lstm model is 74.6% after 30 epochs\n",
      "The training cost for 32 unit lstm model is 0.7420846982435747 after 30 epochs \n",
      "\n",
      "The test accuracy for 32 unit lstm model is 74.3% after 30 epochs\n",
      "The test cost for 32 unit lstm model is 0.7448775470256805 after 30 epochs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the print_acc function to get the correct results\n",
    "print_acc(rnn_size=32, epochs = 30)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
